{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73822ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Найдено файлов для обработки: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "import subprocess\n",
    "\n",
    "#Конфигурация \n",
    "HDFS_NAMENODE = \"rc1a-dataproc-m-j8aqvcff64464qn8.mdb.yandexcloud.net:8020\"\n",
    "HDFS_INPUT_DIR = \"/user/ubuntu/data\"\n",
    "HDFS_OUTPUT_DIR = \"/user/ubuntu/clean/transactions_parquet\"\n",
    "S3_OUTPUT_PATH = \"s3a://otus-bucket-20251311-b1ghiv85eubrk846dis6/clean/transactions_parquet\"\n",
    "S3_ACCESS_KEY = \"\"\n",
    "S3_SECRET_KEY = \"\"\n",
    "\n",
    "# Инициализация Spark с подавлением спилл предупреждений\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"clean_transactions\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"storage.yandexcloud.net\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"3g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.1\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.maxRecordsPerFile\", \"500000\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .config(\"spark.sql.sources.bucketing.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.execution.sortBeforeRepartition\", \"false\") \\\n",
    "    .config(\"spark.sql.execution.useObjectHashAggregateExec\", \"false\") \\\n",
    "    .config(\"spark.sql.execution.fastHashAggregateRowMaxCapacityBit\", \"16\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Схема данных\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"transaction_id\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_datetime\", T.StringType(), nullable=False),\n",
    "    T.StructField(\"customer_id\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"terminal_id\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_amount\", T.DoubleType(), nullable=False),\n",
    "    T.StructField(\"tx_time_seconds\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_time_days\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_fraud\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_fraud_scenario\", T.IntegerType(), nullable=False),\n",
    "])\n",
    "\n",
    "# Получаем список файлов в HDFS INPUT_DIR — будем обрабатывать файлы по одному\n",
    "ls_res = subprocess.run([\"hdfs\", \"dfs\", \"-ls\", HDFS_INPUT_DIR], capture_output=True, text=True)\n",
    "input_files = []\n",
    "if ls_res.returncode == 0:\n",
    "    for ln in ls_res.stdout.strip().splitlines():\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 8:\n",
    "            p = parts[-1]\n",
    "            # normalize path to absolute from HDFS NN if needed\n",
    "            if not p.startswith(\"/\") and not p.startswith(\"hdfs://\"):\n",
    "                p = os.path.join(HDFS_INPUT_DIR, p)\n",
    "            input_files.append(p)\n",
    "else:\n",
    "    print('Не удалось получить список файлов в HDFS INPUT_DIR:', ls_res.stderr[:200])\n",
    "\n",
    "print(f\"→ Найдено файлов для обработки: {len(input_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40421912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " АНАЛИЗ ПРОБЛЕМ КАЧЕСТВА \n",
      "Функция очистки готова: clean_df(df)\n"
     ]
    }
   ],
   "source": [
    "# 2. Анализ и очистка данных\n",
    "print(\" АНАЛИЗ ПРОБЛЕМ КАЧЕСТВА \")\n",
    "\n",
    "# Функция очистки, применяемая к DataFrame\n",
    "def clean_df(df_in):\n",
    "    df_cleaned = df_in.withColumn(\n",
    "        'tx_datetime',\n",
    "        F.to_timestamp(\n",
    "            F.regexp_replace('tx_datetime', ' 24:00:00$', ' 23:59:59'),\n",
    "            'yyyy-MM-dd HH:mm:ss'\n",
    "        )\n",
    "    ).filter(\n",
    "        (F.col('tx_fraud').isNotNull()) &\n",
    "        (F.col('tx_amount') > 0) & (F.col('tx_amount') < 1e7) &\n",
    "        (F.col('tx_time_seconds') >= 0) &\n",
    "        (F.col('tx_time_days') >= 0) &\n",
    "        (F.col('tx_datetime').isNotNull())\n",
    "    ).dropDuplicates(['transaction_id'])\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "print('Функция очистки готова: clean_df(df)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e19e73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== СОХРАНЕНИЕ (HDFS, per-file processing) ===\n",
      "СТАТИСТИКА ПРЕДВАРИТЕЛЬНАЯ:\n",
      "  Файлов для обработки: 40\n",
      "→ Удаляем старую директорию результата (если есть)\n",
      "Отклик удаления выходной директории (может быть не проблема): rm: `/user/ubuntu/clean/transactions_parquet': No such file or directory\n",
      "\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-08-22.txt\n",
      "Отклик удаления выходной директории (может быть не проблема): rm: `/user/ubuntu/clean/transactions_parquet': No such file or directory\n",
      "\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-08-22.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,988,419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,987,353 (removed 1,066)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-08-22.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-09-21.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-08-22.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-09-21.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,994,587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,993,666 (removed 921)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-09-21.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-10-21.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-09-21.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-10-21.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,994,433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,993,489 (removed 944)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-10-21.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-11-20.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-10-21.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-11-20.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,992,240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,991,258 (removed 982)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-11-20.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-12-20.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-11-20.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2019-12-20.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,994,938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,994,074 (removed 864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-12-20.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-01-19.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2019-12-20.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-01-19.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,986,198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,985,282 (removed 916)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-01-19.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-02-18.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-01-19.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-02-18.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,994,272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,993,408 (removed 864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-02-18.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-03-19.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-02-18.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-03-19.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,990,425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,989,546 (removed 879)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-03-19.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-04-18.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-03-19.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-04-18.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 47,001,236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 47,000,295 (removed 941)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-04-18.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-05-18.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-04-18.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-05-18.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,998,003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,996,951 (removed 1,052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-05-18.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-06-17.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-05-18.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-06-17.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,983,064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,982,136 (removed 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-06-17.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-07-17.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-06-17.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-07-17.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 47,000,293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,999,424 (removed 869)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-07-17.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-08-16.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-07-17.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-08-16.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 47,003,160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 47,002,344 (removed 816)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-08-16.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-09-15.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-08-16.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-09-15.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,999,866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,998,973 (removed 893)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-09-15.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-10-15.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-09-15.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-10-15.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 47,001,239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 47,000,241 (removed 998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-10-15.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-11-14.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-10-15.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-11-14.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,995,021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,993,930 (removed 1,091)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-11-14.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-12-14.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-11-14.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2020-12-14.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,983,906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,983,010 (removed 896)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-12-14.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-01-13.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2020-12-14.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-01-13.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,993,756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,992,783 (removed 973)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-01-13.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-02-12.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-01-13.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-02-12.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,997,041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,996,101 (removed 940)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-02-12.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-03-14.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-02-12.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-03-14.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,990,555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,989,671 (removed 884)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-03-14.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-04-13.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-03-14.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-04-13.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,994,571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,993,678 (removed 893)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-04-13.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-05-13.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-04-13.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-05-13.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,992,295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,991,345 (removed 950)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-05-13.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-06-12.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-05-13.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-06-12.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,998,584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,997,524 (removed 1,060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-06-12.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-07-12.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-06-12.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-07-12.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,987,372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,986,527 (removed 845)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-07-12.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-08-11.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-07-12.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-08-11.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,998,056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,997,178 (removed 878)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-08-11.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-09-10.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-08-11.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-09-10.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,994,608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,993,677 (removed 931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-09-10.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-10-10.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-09-10.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-10-10.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,994,094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,993,219 (removed 875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-10-10.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-11-09.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-10-10.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-11-09.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,993,019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,992,109 (removed 910)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-11-09.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-12-09.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-11-09.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2021-12-09.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 47,001,970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 47,001,084 (removed 886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-12-09.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-01-08.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2021-12-09.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-01-08.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,997,816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,996,783 (removed 1,033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-01-08.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-02-07.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-01-08.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-02-07.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 47,005,088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 47,004,180 (removed 908)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-02-07.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-03-09.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-02-07.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-03-09.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,992,442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,991,371 (removed 1,071)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-03-09.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-04-08.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-03-09.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-04-08.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,987,224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,986,180 (removed 1,044)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-04-08.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-05-08.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-04-08.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-05-08.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,993,170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,992,070 (removed 1,100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-05-08.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-06-07.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-05-08.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-06-07.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,992,462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,991,447 (removed 1,015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-06-07.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-07-07.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-06-07.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-07-07.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,996,445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,995,600 (removed 845)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-07-07.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-08-06.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-07-07.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-08-06.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 47,002,233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 47,001,142 (removed 1,091)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-08-06.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-09-05.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-08-06.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-09-05.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,993,905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,992,970 (removed 935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-09-05.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-10-05.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-09-05.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-10-05.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,997,188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,996,267 (removed 921)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-10-05.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-11-04.txt\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-10-05.txt\n",
      "\n",
      "→ Обработка файла: /user/ubuntu/data/2022-11-04.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw rows: 46,998,984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean rows: 46,998,039 (removed 945)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Запись в HDFS выполнена (append)\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-11-04.txt\n",
      "\n",
      "=== ИТОГ ===\n",
      "  Обработано файлов: 40/40\n",
      "  Исходных строк: 1,879,794,178\n",
      "  Очищенных строк: 1,879,756,325\n",
      "  Удалено строк: 37,853\n",
      "  Удалён исходный файл: /user/ubuntu/data/2022-11-04.txt\n",
      "\n",
      "=== ИТОГ ===\n",
      "  Обработано файлов: 40/40\n",
      "  Исходных строк: 1,879,794,178\n",
      "  Очищенных строк: 1,879,756,325\n",
      "  Удалено строк: 37,853\n"
     ]
    }
   ],
   "source": [
    "# === 3. Сохранение в HDFS: последовательная обработка исходных файлов и удаление ===\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\n=== СОХРАНЕНИЕ (HDFS, per-file processing) ===\")\n",
    "\n",
    "print(f\"СТАТИСТИКА ПРЕДВАРИТЕЛЬНАЯ:\")\n",
    "print(f\"  Файлов для обработки: {len(input_files)}\")\n",
    "\n",
    "hdfs_output_full = f\"hdfs://{HDFS_NAMENODE}{HDFS_OUTPUT_DIR}\"\n",
    "\n",
    "# Принудительно очищаем выходной каталог, если нужно (чтобы итог был заменой)\n",
    "print('→ Удаляем старую директорию результата (если есть)')\n",
    "rm_out = subprocess.run([\"hdfs\", \"dfs\", \"-rm\", \"-r\", \"-skipTrash\", HDFS_OUTPUT_DIR], capture_output=True, text=True)\n",
    "if rm_out.returncode == 0:\n",
    "    print('✓ Удалена старая выходная директория HDFS (или её не было)')\n",
    "else:\n",
    "    print(f'Отклик удаления выходной директории (может быть не проблема): {rm_out.stderr[:200]}')\n",
    "\n",
    "# Счетчики для итоговой статистики\n",
    "processed_files = 0\n",
    "total_rows_raw = 0\n",
    "total_rows_clean = 0\n",
    "total_rows_removed = 0\n",
    "\n",
    "for src_path in input_files:\n",
    "    try:\n",
    "        print(f\"\\n→ Обработка файла: {src_path}\")\n",
    "        # Формируем path для чтения Spark (с hdfs://NAMENODE/... если путь начинается с /)\n",
    "        if src_path.startswith('/'):\n",
    "            spark_path = f\"hdfs://{HDFS_NAMENODE}{src_path}\"\n",
    "        elif src_path.startswith('hdfs://'):\n",
    "            spark_path = src_path\n",
    "        else:\n",
    "            spark_path = src_path\n",
    "\n",
    "        # Читаем файл\n",
    "        df_file = spark.read.option('header', 'false').option('sep', ',').schema(schema).csv(spark_path)\n",
    "        raw_cnt = df_file.count()\n",
    "        print(f'  raw rows: {raw_cnt:,}')\n",
    "\n",
    "        # Очищаем\n",
    "        df_file_clean = clean_df(df_file)\n",
    "        clean_cnt = df_file_clean.count()\n",
    "        removed_cnt = raw_cnt - clean_cnt\n",
    "        print(f'  clean rows: {clean_cnt:,} (removed {removed_cnt:,})')\n",
    "\n",
    "        # Записываем в выходной HDFS каталог (append) — последовательная сборка финального набора (замена после предварительного удаления выходной директории)\n",
    "        try:\n",
    "            df_file_clean.coalesce(10).write.format('parquet').mode('append').option('compression', 'snappy').save(hdfs_output_full)\n",
    "            print(' Запись в HDFS выполнена (append)')\n",
    "        except Exception as we:\n",
    "            print('  Ошибка записи в HDFS:', str(we)[:400])\n",
    "            raise\n",
    "\n",
    "        # Удаляем исходный файл из INPUT_DIR только после успешной записи\n",
    "        rm = subprocess.run([\"hdfs\", \"dfs\", \"-rm\", src_path], capture_output=True, text=True)\n",
    "        if rm.returncode == 0:\n",
    "            print(f'  Удалён исходный файл: {src_path}')\n",
    "        else:\n",
    "            print(f' Не удалось удалить исходный файл: {src_path} — stderr: {rm.stderr[:200]}')\n",
    "\n",
    "        # Обновляем глобальные счетчики\n",
    "        processed_files += 1\n",
    "        total_rows_raw += raw_cnt\n",
    "        total_rows_clean += clean_cnt\n",
    "        total_rows_removed += removed_cnt\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f'Ошибка при обработке файла {src_path}: {str(ex)[:400]}')\n",
    "        print('  → Пропускаем файл (не удаляем исходный) и переходим к следующему')\n",
    "        continue\n",
    "\n",
    "# Итог\n",
    "print('\\n=== ИТОГ ===')\n",
    "print(f'  Обработано файлов: {processed_files}/{len(input_files)}')\n",
    "print(f'  Исходных строк: {total_rows_raw:,}')\n",
    "print(f'  Очищенных строк: {total_rows_clean:,}')\n",
    "print(f'  Удалено строк: {total_rows_removed:,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c31ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Копируем в S3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Копируем в S3...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Копирование в S3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m→ Копируем в S3...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhadoop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistcp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-overwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHDFS_OUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS3_OUTPUT_PATH\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7200\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ S3: готово\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:495\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    497\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1028\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1028\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1868\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   1862\u001b[0m                         stdout, stderr,\n\u001b[1;32m   1863\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   1865\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1866\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1868\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   1872\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Копирование в S3\n",
    "print(f\"→ Копируем в S3...\")\n",
    "result = subprocess.run(\n",
    "    [\"hadoop\", \"distcp\", \"-overwrite\", \"-m\", \"32\", HDFS_OUTPUT_DIR, S3_OUTPUT_PATH],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    timeout=7200\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ S3: готово\")\n",
    "else:\n",
    "    print(f\"S3 копирование: {result.stderr[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# копирование с S3 в HDFS\n",
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    'hadoop', 'distcp', '-overwrite', '-m', '32', S3_OUTPUT_PATH, HDFS_OUTPUT_DIR\n",
    "]\n",
    "result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)\n",
    "if result.returncode == 0:\n",
    "    print('✓ Обратное копирование: S3 -> HDFS выполнено успешно')\n",
    "else:\n",
    "    print(f\"Ошибка обратного копирования (S3 -> HDFS): {result.stderr[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a72ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скрипт создан: /home/ubuntu/clean_transactions.py\n",
      "Запуск: spark-submit /home/ubuntu/clean_transactions.py\n"
     ]
    }
   ],
   "source": [
    "# === 5. Создание standalone скрипта (composed: init, clean, HDFS write, distcp in S3) ===\n",
    "script_path = \"/home/ubuntu/clean_transactions.py\"\n",
    "\n",
    "# Исправленный шаблон скрипта — обычная строка (не f-string), корректные тройные кавычки внутри\n",
    "script_template = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Скрипт очистки транзакционных данных и выгрузки в S3.\n",
    "Шаги:\n",
    "  1) Инициализация Spark (с локальными JAR'ами в SPARK_JARS)\n",
    "  2) Получение списка файлов в HDFS_INPUT_DIR\n",
    "  3) По одному: читаем, чистим, append в HDFS_OUTPUT_DIR, удаляем исходный файл\n",
    "  4) Финальное hadoop distcp HDFS_OUTPUT_DIR -> S3_OUTPUT_PATH\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "SPARK_JARS = \"/usr/lib/spark/jars/hadoop-aws-3.3.4.jar,/usr/lib/spark/jars/aws-java-sdk-bundle-1.11.1026.jar\"\n",
    "# SPARK_PACKAGES removed — используем локальные JAR'ы, скачанные Ansible'ом\n",
    "\n",
    "def clean_df(df_in):\n",
    "    return df_in.withColumn(\n",
    "        'tx_datetime',\n",
    "        F.to_timestamp(\n",
    "            F.regexp_replace('tx_datetime', ' 24:00:00$', ' 23:59:59'),\n",
    "            'yyyy-MM-dd HH:mm:ss'\n",
    "        )\n",
    "    ).filter(\n",
    "        (F.col('tx_fraud').isNotNull()) &\n",
    "        (F.col('tx_amount') > 0) & (F.col('tx_amount') < 1e7) &\n",
    "        (F.col('tx_time_seconds') >= 0) &\n",
    "        (F.col('tx_time_days') >= 0) &\n",
    "        (F.col('tx_datetime').isNotNull())\n",
    "    ).dropDuplicates(['transaction_id'])\n",
    "\n",
    "def list_hdfs_files(hdfs_input_dir):\n",
    "    r = subprocess.run(['hdfs', 'dfs', '-ls', hdfs_input_dir], capture_output=True, text=True)\n",
    "    files = []\n",
    "    if r.returncode != 0:\n",
    "        print('Не удалось получить список файлов в HDFS:', r.stderr[:300], file=sys.stderr)\n",
    "        return files\n",
    "    for ln in r.stdout.strip().splitlines():\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 8:\n",
    "            files.append(parts[-1])\n",
    "    return files\n",
    "\n",
    "def main():\n",
    "    # Параметры (подставляются .replace)\n",
    "    HDFS_NAMENODE = '{HDFS_NAMENODE}'\n",
    "    HDFS_INPUT_DIR = '{HDFS_INPUT_DIR}'\n",
    "    HDFS_OUTPUT_DIR = '{HDFS_OUTPUT_DIR}'\n",
    "    S3_OUTPUT_PATH = '{S3_OUTPUT_PATH}'\n",
    "    S3_ACCESS_KEY = '{S3_ACCESS_KEY}'\n",
    "    S3_SECRET_KEY = '{S3_SECRET_KEY}'\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('clean_transactions') \\\n",
    "        .config('spark.jars', SPARK_JARS) \\\n",
    "        .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "        .config('spark.hadoop.fs.s3a.access.key', S3_ACCESS_KEY) \\\n",
    "        .config('spark.hadoop.fs.s3a.secret.key', S3_SECRET_KEY) \\\n",
    "        .config('spark.hadoop.fs.s3a.endpoint', 'storage.yandexcloud.net') \\\n",
    "        .config('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField('transaction_id', T.IntegerType(), nullable=False),\n",
    "        T.StructField('tx_datetime', T.StringType(), nullable=False),\n",
    "        T.StructField('customer_id', T.IntegerType(), nullable=False),\n",
    "        T.StructField('terminal_id', T.IntegerType(), nullable=False),\n",
    "        T.StructField('tx_amount', T.DoubleType(), nullable=False),\n",
    "        T.StructField('tx_time_seconds', T.IntegerType(), nullable=False),\n",
    "        T.StructField('tx_time_days', T.IntegerType(), nullable=False),\n",
    "        T.StructField('tx_fraud', T.IntegerType(), nullable=False),\n",
    "        T.StructField('tx_fraud_scenario', T.IntegerType(), nullable=False),\n",
    "    ])\n",
    "\n",
    "    files = list_hdfs_files(HDFS_INPUT_DIR)\n",
    "    print(f'Files to process: {len(files)}')\n",
    "    if not files:\n",
    "        print('No files found in HDFS input dir, exiting')\n",
    "        spark.stop()\n",
    "        return\n",
    "\n",
    "    # Очистим выходной каталог заранее (чтобы итог был заменой)\n",
    "    subprocess.run(['hdfs','dfs','-rm','-r','-skipTrash', HDFS_OUTPUT_DIR], capture_output=True)\n",
    "\n",
    "    processed = 0\n",
    "    total_raw = 0\n",
    "    total_clean = 0\n",
    "    for src in files:\n",
    "        try:\n",
    "            print('Processing', src)\n",
    "            if src.startswith('/'):\n",
    "                reader = f'hdfs://{HDFS_NAMENODE}' + src\n",
    "            else:\n",
    "                reader = src\n",
    "            df_raw = spark.read.option('header','false').option('sep',',').schema(schema).csv(reader)\n",
    "            cnt_raw = df_raw.count()\n",
    "            df_clean = clean_df(df_raw)\n",
    "            cnt_clean = df_clean.count()\n",
    "            df_clean.coalesce(10).write.format('parquet').mode('append').option('compression','snappy').save(f'hdfs://{HDFS_NAMENODE}{HDFS_OUTPUT_DIR}')\n",
    "            # Только после успешной записи удаляем исходный файл\n",
    "            subprocess.run(['hdfs','dfs','-rm', src], capture_output=True)\n",
    "            print(f'{src}: {cnt_raw:,} -> {cnt_clean:,} processed')\n",
    "            processed += 1\n",
    "            total_raw += cnt_raw\n",
    "            total_clean += cnt_clean\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {src}: {str(e)[:400]}', file=sys.stderr)\n",
    "            # не удаляем исходник при ошибке; переходим к следующему\n",
    "            continue\n",
    "\n",
    "    print('Processing complete')\n",
    "    print(f'Files processed: {processed}/{len(files)}')\n",
    "    print(f'Rows: {total_raw:,} -> {total_clean:,}')\n",
    "\n",
    "    # Финальное distcp в S3\n",
    "    try:\n",
    "        print('Starting distcp to S3...')\n",
    "        rc = subprocess.run(['hadoop','distcp','-overwrite','-m','64', f'hdfs://{HDFS_NAMENODE}{HDFS_OUTPUT_DIR}', S3_OUTPUT_PATH], capture_output=True, text=True, timeout=7200)\n",
    "        if rc.returncode == 0:\n",
    "            print('✓ distcp to S3 completed successfully')\n",
    "        else:\n",
    "            print('⚠ distcp failed:', rc.stderr[:400])\n",
    "    except Exception as e:\n",
    "        print('⚠ distcp exception:', str(e)[:400])\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# подменяем плейсхолдеры\n",
    "script_content = script_template.replace('{HDFS_NAMENODE}', HDFS_NAMENODE) \\\n",
    "    .replace('{HDFS_INPUT_DIR}', HDFS_INPUT_DIR) \\\n",
    "    .replace('{HDFS_OUTPUT_DIR}', HDFS_OUTPUT_DIR) \\\n",
    "    .replace('{S3_OUTPUT_PATH}', S3_OUTPUT_PATH) \\\n",
    "    .replace('{S3_ACCESS_KEY}', S3_ACCESS_KEY) \\\n",
    "    .replace('{S3_SECRET_KEY}', S3_SECRET_KEY)\n",
    "\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "import subprocess\n",
    "subprocess.run(['chmod', '+x', script_path])\n",
    "\n",
    "print(f\"Скрипт создан: {script_path}\")\n",
    "print(f\"Запуск: spark-submit {script_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
