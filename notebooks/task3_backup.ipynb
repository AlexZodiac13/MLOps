{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73822ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:===================================================>     (19 + 2) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ: 46,988,419 —Å—Ç—Ä–æ–∫ –∏–∑ HDFS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "#–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è \n",
    "HDFS_NAMENODE = \"rc1a-dataproc-m-0tvi3u0a47j8dj6t.mdb.yandexcloud.net:8020\"\n",
    "HDFS_INPUT_DIR = \"/user/ubuntu/data\"\n",
    "HDFS_OUTPUT_DIR = \"/user/ubuntu/clean/transactions_parquet\"\n",
    "S3_OUTPUT_PATH = \"s3a://otus-bucket-20251311-b1ghiv85eubrk846dis6/clean/transactions_parquet\"\n",
    "S3_ACCESS_KEY = \"-\"\n",
    "S3_SECRET_KEY = \"-\"\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark —Å –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ–º —Å–ø–∏–ª–ª –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"clean_transactions\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"storage.yandexcloud.net\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"3g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.1\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.maxRecordsPerFile\", \"500000\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .config(\"spark.sql.sources.bucketing.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.execution.sortBeforeRepartition\", \"false\") \\\n",
    "    .config(\"spark.sql.execution.useObjectHashAggregateExec\", \"false\") \\\n",
    "    .config(\"spark.sql.execution.fastHashAggregateRowMaxCapacityBit\", \"16\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# –°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"transaction_id\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_datetime\", T.StringType(), nullable=False),\n",
    "    T.StructField(\"customer_id\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"terminal_id\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_amount\", T.DoubleType(), nullable=False),\n",
    "    T.StructField(\"tx_time_seconds\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_time_days\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_fraud\", T.IntegerType(), nullable=False),\n",
    "    T.StructField(\"tx_fraud_scenario\", T.IntegerType(), nullable=False),\n",
    "])\n",
    "\n",
    "# –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ HDFS\n",
    "input_path = f\"hdfs://{HDFS_NAMENODE}{HDFS_INPUT_DIR}/*\"\n",
    "df_raw = spark.read.option(\"header\", \"false\").option(\"sep\", \",\").schema(schema).csv(input_path)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫\n",
    "total_raw = df_raw.count()\n",
    "print(f\"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {total_raw:,} —Å—Ç—Ä–æ–∫ –∏–∑ HDFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40421912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== –ê–ù–ê–õ–ò–ó –ü–†–û–ë–õ–ï–ú –ö–ê–ß–ï–°–¢–í–ê ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. –ü–†–û–ü–£–©–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. –ù–ï–ö–û–†–†–ï–ö–¢–ù–´–ï –°–£–ú–ú–´: 884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. –ù–ï–ö–û–†–†–ï–ö–¢–ù–û–ï –í–†–ï–ú–Ø: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. –î–£–ë–õ–ò–ö–ê–¢–´: 181\n",
      "\n",
      "=== –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–• ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì –û—á–∏—â–µ–Ω–æ: 46,987,353 —Å—Ç—Ä–æ–∫ (100.00%)\n",
      "‚úì –£–¥–∞–ª–µ–Ω–æ: 1,066 —Å—Ç—Ä–æ–∫ (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[transaction_id: int, tx_datetime: timestamp, customer_id: int, terminal_id: int, tx_amount: double, tx_time_seconds: int, tx_time_days: int, tx_fraud: int, tx_fraud_scenario: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 2. –ê–Ω–∞–ª–∏–∑ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===\n",
    "print(\"\\n=== –ê–ù–ê–õ–ò–ó –ü–†–û–ë–õ–ï–ú –ö–ê–ß–ï–°–¢–í–ê ===\")\n",
    "\n",
    "# –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º\n",
    "print(f\"1. –ü–†–û–ü–£–©–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø: {df_raw.filter(F.col('tx_fraud').isNull()).count()}\")\n",
    "print(f\"2. –ù–ï–ö–û–†–†–ï–ö–¢–ù–´–ï –°–£–ú–ú–´: {df_raw.filter(F.col('tx_amount') <= 0).count()}\")\n",
    "print(f\"3. –ù–ï–ö–û–†–†–ï–ö–¢–ù–û–ï –í–†–ï–ú–Ø: {df_raw.filter(F.col('tx_datetime').rlike('24:00:00')).count()}\")\n",
    "\n",
    "# –î—É–±–ª–∏–∫–∞—Ç—ã\n",
    "total_records = df_raw.count()\n",
    "unique_records = df_raw.select(\"transaction_id\").distinct().count()\n",
    "duplicates = total_records - unique_records\n",
    "print(f\"4. –î–£–ë–õ–ò–ö–ê–¢–´: {duplicates}\")\n",
    "\n",
    "print(f\"\\n=== –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–• ===\")\n",
    "\n",
    "# –û—á–∏—Å—Ç–∫–∞\n",
    "df = df_raw.withColumn(\n",
    "    \"tx_datetime\",\n",
    "    F.to_timestamp(\n",
    "        F.regexp_replace(\"tx_datetime\", \" 24:00:00$\", \" 23:59:59\"),\n",
    "        \"yyyy-MM-dd HH:mm:ss\"\n",
    "    )\n",
    ").filter(\n",
    "    (F.col(\"tx_fraud\").isNotNull()) &\n",
    "    (F.col(\"tx_amount\") > 0) & \n",
    "    (F.col(\"tx_amount\") < 1e7) &\n",
    "    (F.col(\"tx_time_seconds\") >= 0) &\n",
    "    (F.col(\"tx_time_days\") >= 0) &\n",
    "    (F.col(\"tx_datetime\").isNotNull())\n",
    ").dropDuplicates([\"transaction_id\"])\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\n",
    "total_clean = df.count()\n",
    "removed = total_raw - total_clean\n",
    "removed_pct = (removed / total_raw) * 100 if total_raw > 0 else 0\n",
    "\n",
    "print(f\"‚úì –û—á–∏—â–µ–Ω–æ: {total_clean:,} —Å—Ç—Ä–æ–∫ ({100*total_clean/total_raw:.2f}%)\")\n",
    "print(f\"‚úì –£–¥–∞–ª–µ–Ω–æ: {removed:,} —Å—Ç—Ä–æ–∫ ({removed_pct:.2f}%)\")\n",
    "\n",
    "# –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ü–û–°–õ–ï –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Å df  \n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e19e73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== –°–û–•–†–ê–ù–ï–ù–ò–ï ===\n",
      "\n",
      "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ß–ò–°–¢–ö–ò:\n",
      "  –ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π:    46,988,419\n",
      "  –û—á–∏—â–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π:   46,987,353\n",
      "  –£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π:     1,066 (0.00%)\n",
      "  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ:           100.00%\n",
      "\n",
      "‚Üí –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ HDFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì HDFS: –≥–æ—Ç–æ–≤–æ\n",
      "‚Üí –ö–æ–ø–∏—Ä—É–µ–º –≤ S3...\n",
      "‚úì S3: –≥–æ—Ç–æ–≤–æ\n"
     ]
    }
   ],
   "source": [
    "# === 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ HDFS –∏ S3 ===\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\n=== –°–û–•–†–ê–ù–ï–ù–ò–ï ===\")\n",
    "\n",
    "print(f\"\\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ß–ò–°–¢–ö–ò:\")\n",
    "print(f\"  –ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π:    {total_raw:,}\")\n",
    "print(f\"  –û—á–∏—â–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π:   {total_clean:,}\")\n",
    "print(f\"  –£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π:     {removed:,} ({removed_pct:.2f}%)\")\n",
    "print(f\"  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ:           {100*total_clean/total_raw:.2f}%\")\n",
    "\n",
    "hdfs_path = f\"hdfs://{HDFS_NAMENODE}{HDFS_OUTPUT_DIR}\"\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π HDFS\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-mkdir\", \"-p\", HDFS_OUTPUT_DIR], capture_output=True)\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-rm\", \"-r\", HDFS_OUTPUT_DIR], capture_output=True)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ HDFS\n",
    "print(f\"\\n‚Üí –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ HDFS...\")\n",
    "df.coalesce(10).write.format(\"parquet\").mode(\"overwrite\").save(hdfs_path)\n",
    "print(\"‚úì HDFS: –≥–æ—Ç–æ–≤–æ\")\n",
    "\n",
    "# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ S3\n",
    "print(f\"‚Üí –ö–æ–ø–∏—Ä—É–µ–º –≤ S3...\")\n",
    "result = subprocess.run(\n",
    "    [\"hadoop\", \"distcp\", \"-overwrite\", \"-m\", \"32\", HDFS_OUTPUT_DIR, S3_OUTPUT_PATH],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    timeout=1200\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úì S3: –≥–æ—Ç–æ–≤–æ\")\n",
    "else:\n",
    "    print(f\"‚ö† S3 –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ: {result.stderr[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f62b79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞–Ω: /home/ubuntu/clean_transactions.py\n",
      "üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–∑—è—Ç–∞ –∏–∑ Cell 1:\n",
      "   ‚Ä¢ HDFS: rc1a-dataproc-m-0tvi3u0a47j8dj6t.mdb.yandexcloud.net:8020\n",
      "   ‚Ä¢ Bucket: s3a://otus-bucket-20251311-b1ghiv85eubrk846dis6/clean/transactions_parquet\n",
      "   ‚Ä¢ Keys: YCAJEBaG2V...\n",
      "\n",
      "üöÄ –ó–∞–ø—É—Å–∫:\n",
      "   spark-submit /home/ubuntu/clean_transactions.py\n"
     ]
    }
   ],
   "source": [
    "# === 4. –°–æ–∑–¥–∞–Ω–∏–µ standalone —Å–∫—Ä–∏–ø—Ç–∞ ===\n",
    "script_path = \"/home/ubuntu/clean_transactions.py\"\n",
    "\n",
    "# –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∫—Ä–∏–ø—Ç –∏—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ –∏–∑ Cell 1\n",
    "script_content = f'''#!/usr/bin/env python3\n",
    "\"\"\"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è MLOps.\"\"\"\n",
    "import subprocess\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "def main():\n",
    "    # –°–æ–∑–¥–∞–µ–º Spark —Å–µ—Å—Å–∏—é —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏\n",
    "    spark = SparkSession.builder \\\\\n",
    "        .appName(\"clean_transactions\") \\\\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"{S3_ACCESS_KEY}\") \\\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"{S3_SECRET_KEY}\") \\\\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"storage.yandexcloud.net\") \\\\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"400\") \\\\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"3g\") \\\\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.1\") \\\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # –ü–æ–¥–∞–≤–ª—è–µ–º WARN —Å–æ–æ–±—â–µ–Ω–∏—è\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–∑—è—Ç–∞ –∏–∑ notebook)\n",
    "    HDFS_NAMENODE = \"{HDFS_NAMENODE}\"\n",
    "    HDFS_INPUT_DIR = \"{HDFS_INPUT_DIR}\"\n",
    "    HDFS_OUTPUT_DIR = \"{HDFS_OUTPUT_DIR}\"\n",
    "    S3_OUTPUT_PATH = \"{S3_OUTPUT_PATH}\"\n",
    "\n",
    "    # –°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"transaction_id\", T.IntegerType(), nullable=False),\n",
    "        T.StructField(\"tx_datetime\", T.StringType(), nullable=False),\n",
    "        T.StructField(\"customer_id\", T.IntegerType(), nullable=False),\n",
    "        T.StructField(\"terminal_id\", T.IntegerType(), nullable=False),\n",
    "        T.StructField(\"tx_amount\", T.DoubleType(), nullable=False),\n",
    "        T.StructField(\"tx_time_seconds\", T.IntegerType(), nullable=False),\n",
    "        T.StructField(\"tx_time_days\", T.IntegerType(), nullable=False),\n",
    "        T.StructField(\"tx_fraud\", T.IntegerType(), nullable=False),\n",
    "        T.StructField(\"tx_fraud_scenario\", T.IntegerType(), nullable=False),\n",
    "    ])\n",
    "\n",
    "    print(\"=== –ù–ê–ß–ê–õ–û –û–ß–ò–°–¢–ö–ò –î–ê–ù–ù–´–• ===\")\n",
    "    \n",
    "    # 1. –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ HDFS\n",
    "    input_path = f\"hdfs://{{HDFS_NAMENODE}}{{HDFS_INPUT_DIR}}/*\"\n",
    "    df_raw = spark.read.option(\"header\", \"false\").option(\"sep\", \",\").schema(schema).csv(input_path)\n",
    "    \n",
    "    total_raw = df_raw.count()\n",
    "    print(f\"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {{total_raw:,}} —Å—Ç—Ä–æ–∫ –∏–∑ HDFS\")\n",
    "\n",
    "    # 2. –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    print(\"\\\\n=== –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê ===\")\n",
    "    null_fraud = df_raw.filter(F.col('tx_fraud').isNull()).count()\n",
    "    invalid_amount = df_raw.filter(F.col('tx_amount') <= 0).count()\n",
    "    invalid_time = df_raw.filter(F.col('tx_datetime').rlike('24:00:00')).count()\n",
    "    \n",
    "    total_records = df_raw.count()\n",
    "    unique_records = df_raw.select(\"transaction_id\").distinct().count()\n",
    "    duplicates = total_records - unique_records\n",
    "    \n",
    "    print(f\"‚Ä¢ NULL –∑–Ω–∞—á–µ–Ω–∏—è tx_fraud: {{null_fraud}}\")\n",
    "    print(f\"‚Ä¢ –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å—É–º–º—ã: {{invalid_amount}}\")\n",
    "    print(f\"‚Ä¢ –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –≤—Ä–µ–º—è: {{invalid_time}}\")\n",
    "    print(f\"‚Ä¢ –î—É–±–ª–∏–∫–∞—Ç—ã: {{duplicates}}\")\n",
    "\n",
    "    # 3. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    print(\"\\\\n=== –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–• ===\")\n",
    "    df_clean = df_raw.withColumn(\n",
    "        \"tx_datetime\",\n",
    "        F.to_timestamp(\n",
    "            F.regexp_replace(\"tx_datetime\", \" 24:00:00$\", \" 23:59:59\"),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    ).filter(\n",
    "        (F.col(\"tx_fraud\").isNotNull()) &\n",
    "        (F.col(\"tx_amount\") > 0) & (F.col(\"tx_amount\") < 1e7) &\n",
    "        (F.col(\"tx_time_seconds\") >= 0) &\n",
    "        (F.col(\"tx_time_days\") >= 0) &\n",
    "        (F.col(\"tx_datetime\").isNotNull())\n",
    "    ).dropDuplicates([\"transaction_id\"])\n",
    "    \n",
    "    total_clean = df_clean.count()\n",
    "    removed = total_raw - total_clean\n",
    "    removed_pct = (removed / total_raw) * 100 if total_raw > 0 else 0\n",
    "    \n",
    "    print(f\"‚úì –û—á–∏—â–µ–Ω–æ: {{total_clean:,}} —Å—Ç—Ä–æ–∫ ({{100*total_clean/total_raw:.2f}}%)\")\n",
    "    print(f\"‚úì –£–¥–∞–ª–µ–Ω–æ: {{removed:,}} —Å—Ç—Ä–æ–∫ ({{removed_pct:.2f}}%)\")\n",
    "\n",
    "    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ HDFS\n",
    "    print(\"\\\\n=== –°–û–•–†–ê–ù–ï–ù–ò–ï ===\")\n",
    "    hdfs_path = f\"hdfs://{{HDFS_NAMENODE}}{{HDFS_OUTPUT_DIR}}\"\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n",
    "    subprocess.run([\"hdfs\", \"dfs\", \"-mkdir\", \"-p\", HDFS_OUTPUT_DIR], capture_output=True)\n",
    "    subprocess.run([\"hdfs\", \"dfs\", \"-rm\", \"-r\", HDFS_OUTPUT_DIR], capture_output=True)\n",
    "    \n",
    "    print(\"‚Üí –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ HDFS...\")\n",
    "    df_clean.coalesce(10).write.format(\"parquet\").mode(\"overwrite\").save(hdfs_path)\n",
    "    print(\"‚úì HDFS: –≥–æ—Ç–æ–≤–æ\")\n",
    "    \n",
    "    # 5. –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ S3\n",
    "    print(\"‚Üí –ö–æ–ø–∏—Ä—É–µ–º –≤ S3...\")\n",
    "    result = subprocess.run(\n",
    "        [\"hadoop\", \"distcp\", \"-overwrite\", \"-m\", \"32\", HDFS_OUTPUT_DIR, S3_OUTPUT_PATH],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=1800\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úì S3: –≥–æ—Ç–æ–≤–æ\")\n",
    "    else:\n",
    "        print(f\"‚ö† S3 –æ—à–∏–±–∫–∞: {{result.stderr[:200]}}\")\n",
    "    \n",
    "    print(f\"\\\\n=== –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===\")\n",
    "    print(f\"‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {{total_raw:,}} ‚Üí {{total_clean:,}} –∑–∞–ø–∏—Å–µ–π\")\n",
    "    print(f\"‚Ä¢ HDFS –ø—É—Ç—å: {{HDFS_OUTPUT_DIR}}\")\n",
    "    print(f\"‚Ä¢ S3 –ø—É—Ç—å: {{S3_OUTPUT_PATH}}\")\n",
    "    print(\"=== –ó–ê–í–ï–†–®–ï–ù–û ===\")\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º —Å–∫—Ä–∏–ø—Ç\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "# –î–µ–ª–∞–µ–º –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–º\n",
    "import subprocess\n",
    "subprocess.run(['chmod', '+x', script_path])\n",
    "    \n",
    "print(f\"‚úÖ –°–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞–Ω: {script_path}\")\n",
    "print(f\"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–∑—è—Ç–∞ –∏–∑ Cell 1:\")\n",
    "print(f\"   ‚Ä¢ HDFS: {HDFS_NAMENODE}\")\n",
    "print(f\"   ‚Ä¢ Bucket: {S3_OUTPUT_PATH}\")\n",
    "print(f\"   ‚Ä¢ Keys: {S3_ACCESS_KEY[:10]}...\") \n",
    "print(f\"\\nüöÄ –ó–∞–ø—É—Å–∫:\")\n",
    "print(f\"   spark-submit {script_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
