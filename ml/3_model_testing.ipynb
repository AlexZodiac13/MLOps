{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fac3205",
   "metadata": {},
   "source": [
    "# 3. Model Testing (Inference Simulation)\n",
    "\n",
    "This notebook simulates the inference process on the target hardware (4 CPU, 4GB RAM).\n",
    "We assume the model has been converted to GGUF format (e.g., `qwen2.5-3b-reminder-bot-q4_k_m.gguf`).\n",
    "\n",
    "If the GGUF file is not ready, this notebook will fail. \n",
    "You can use `llama-cpp-python` to run GGUF models in Python.\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install llama-cpp-python\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d2bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 28 key-value pairs and 434 tensors from qwen2.5-3b-reminder-bot-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20\n",
      "llama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.800000\n",
      "llama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.700000\n",
      "llama_model_loader: - kv   5:                               general.name str              = Qwen2.5 3b Reminder Bot_Merged\n",
      "llama_model_loader: - kv   6:                           general.finetune str              = reminder-bot_merged\n",
      "llama_model_loader: - kv   7:                           general.basename str              = qwen2.5\n",
      "llama_model_loader: - kv   8:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   9:                          qwen2.block_count u32              = 36\n",
      "llama_model_loader: - kv  10:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     qwen2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  13:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  14:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  15:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  16:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151645\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  27:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q4_K:  216 tensors\n",
      "llama_model_loader: - type q6_K:   37 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.79 GiB (4.99 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 151643 ('<|endoftext|>')\n",
      "load:   - 151645 ('<|im_end|>')\n",
      "load:   - 151662 ('<|fim_pad|>')\n",
      "load:   - 151663 ('<|repo_name|>')\n",
      "load:   - 151664 ('<|file_sep|>')\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = -1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5 3b Reminder Bot_Merged\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151645 '<|im_end|>'\n",
      "print_info: LF token         = 198 'ƒä'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 218 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  1265.62 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  1822.72 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.32.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.32.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.32.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.32.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.32.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.33.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.33.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.33.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.33.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.33.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.34.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.34.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.34.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.34.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.34.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.35.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.35.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.35.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.35.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.35.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified: layer  34: dev = CPU\n",
      "llama_kv_cache_unified: layer  35: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    72.00 MiB\n",
      "llama_kv_cache_unified: size =   72.00 MiB (  2048 cells,  36 layers,  1/1 seqs), K (f16):   36.00 MiB, V (f16):   36.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 3472\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   300.75 MiB\n",
      "llama_context: graph nodes  = 1374\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'qwen2.attention.head_count': '16', 'general.architecture': 'qwen2', 'general.sampling.top_k': '20', 'tokenizer.ggml.padding_token_id': '151645', 'general.basename': 'qwen2.5', 'qwen2.embedding_length': '2048', 'general.sampling.top_p': '0.800000', 'qwen2.context_length': '32768', 'general.sampling.temp': '0.700000', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 3b Reminder Bot_Merged', 'qwen2.block_count': '36', 'general.finetune': 'reminder-bot_merged', 'general.type': 'model', 'general.size_label': '3B', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '11008'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on CPU.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import json\n",
    "\n",
    "# Path to your GGUF model\n",
    "# You must have run the conversion in step 2 or downloaded a GGUF version\n",
    "MODEL_PATH = \"qwen2.5-3b-reminder-bot-q4_k_m.gguf\" \n",
    "\n",
    "try:\n",
    "    # Initialize model with 4GB RAM constraint logic\n",
    "    # n_ctx=2048 context window\n",
    "    llm = Llama(\n",
    "        model_path=MODEL_PATH,\n",
    "        n_ctx=2048,\n",
    "        n_threads=4,      # 4 CPU cores\n",
    "        n_gpu_layers=0    # 0 GPU layers (CPU only simulation)\n",
    "    )\n",
    "    print(\"Model loaded successfully on CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model (Did you convert it to GGUF?): {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de27f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 random messages from CSV.\n",
      "Input: –ö–ê–ñ–î–û–ï 28 –ß–ò–°–õ–û –ü–†–û–í–ï–†–ö–ê –ü–†–ï–ú–ò–£–ú–ê –¢–ò–ù–¨–ö–û–§–§\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     516.51 ms /   164 tokens (    3.15 ms per token,   317.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2121.81 ms /    53 runs   (   40.03 ms per token,    24.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    2683.81 ms /   217 tokens\n",
      "llama_perf_context_print:    graphs reused =         51\n",
      "Llama.generate: 127 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–ü–†–û–í–ï–†–ö–ê –ü–†–ï–ú–ò–£–ú–ê –¢–ò–ù–¨–ö–û–§–§\", \"date\": \"2026-02-28\", \"time\": \"00:00\", \"repeat\": \"monthly\"}\n",
      "--------------------\n",
      "Input: 16 –§–ï–í–†–ê–õ–Ø –ò–¢–û–ì–ò –†–û–ó–´–ì–†–´–®–ê\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     121.68 ms /    29 tokens (    4.20 ms per token,   238.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1853.27 ms /    46 runs   (   40.29 ms per token,    24.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    2007.37 ms /    75 tokens\n",
      "llama_perf_context_print:    graphs reused =         43\n",
      "Llama.generate: 127 prefix-match hit, remaining 28 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–ò—Ç–æ–≥–∏ —Ä–æ–∑—ã–≥—Ä—ã—à–∞\", \"date\": \"2026-02-16\", \"time\": \"00:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: –ö–ê–ñ–î–û–ï 1 –ß–ò–°–õ–û –û–ü–õ–ê–¢–ê –ü–û–ß–¢–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     114.61 ms /    28 tokens (    4.09 ms per token,   244.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1665.70 ms /    41 runs   (   40.63 ms per token,    24.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    1808.68 ms /    69 tokens\n",
      "llama_perf_context_print:    graphs reused =         38\n",
      "Llama.generate: 127 prefix-match hit, remaining 18 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–û–ø–ª–∞—Ç–∞ –ø–æ—á—Ç—ã\", \"date\": \"2026-02-01\", \"time\": \"00:00\", \"repeat\": \"monthly\"}\n",
      "--------------------\n",
      "Input: 10-00 –°–¥–µ–ª–∞–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =      96.04 ms /    18 tokens (    5.34 ms per token,   187.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1638.23 ms /    41 runs   (   39.96 ms per token,    25.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1761.94 ms /    59 tokens\n",
      "llama_perf_context_print:    graphs reused =         39\n",
      "Llama.generate: 126 prefix-match hit, remaining 208 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–°–¥–µ–ª–∞–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\", \"date\": \"2026-02-18\", \"time\": \"10:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: [–†–µ–ª–∏–∑—ã]\n",
      "https ://wiki.domrf.ru/pages/viewpage.action?pageId=267595334\n",
      "\n",
      "–ë–∞–Ω–∫:\n",
      "‚ÄºÔ∏è12,12.2025 –≤ 11:00 (–ø—è—Ç–Ω–∏—Ü–∞)\n",
      "https://jira.domrf.ru/browse/INTEGRATIO-4766\n",
      "Paycontrol - –¥–æ—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥–ø–∏—Å–∏\n",
      "https://jira.domrf.ru/browse/RFC-9711\n",
      "@hopheylalalei —Å–¥–µ–ª–∞–π —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ —ç—Ç–æ–º—É —Ä–µ–ª–∏–∑—É –∏ –ø—Ä–æ–ø–∏—à–∏ –≤–µ—Ç–∫–∏ –ø–ª–∑ –≤ —Ç–∞–±–ª–∏—á–∫–µ https://wiki.domrf.ru/pages/viewpage.action?pageId=315611369 \n",
      " [elk-facade] [paycontrol-adapter] –ï–õ–ö-–§–ê–°–ê–î –≤–µ—Ç–∫–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å @Pajadla\n",
      "\n",
      "@alexzodiac @PristTech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     648.19 ms /   208 tokens (    3.12 ms per token,   320.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2474.96 ms /    59 runs   (   41.95 ms per token,    23.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    3169.06 ms /   267 tokens\n",
      "llama_perf_context_print:    graphs reused =         56\n",
      "Llama.generate: 161 prefix-match hit, remaining 138 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"‚ÄºÔ∏è12,12.2025 –≤ 11:00 (–ø—è—Ç–Ω–∏—Ü–∞)\", \"date\": \"2025-12-12\", \"time\": \"11:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: [–†–µ–ª–∏–∑—ã]\n",
      "https ://wiki.domrf.ru/pages/viewpage.action?pageId=267595334\n",
      "\n",
      "–ë–∞–Ω–∫:\n",
      "‚ÄºÔ∏è20.01.2026 –≤ 22:00 (@TurovVal)\n",
      "https://jira.domrf.ru/browse/INTEGRATIO-3935\n",
      "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ mq\n",
      "https://jira.domrf.ru/browse/RFC-9919\n",
      "@alexandra00001 –ø—Ä–æ–ø–∏—à–∏ —Ä–µ–ª–∏–∑–Ω–æ—É—Ç –∏ –≤–µ—Ç–∫–∏ –ø–ª–∑ https://wiki.domrf.ru/pages/viewpage.action?pageId=315611369 \n",
      "\n",
      "[rbs-paydocs]\n",
      "\n",
      "@alexzodiac @PristTech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     568.68 ms /   138 tokens (    4.12 ms per token,   242.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2644.27 ms /    65 runs   (   40.68 ms per token,    24.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    3257.53 ms /   203 tokens\n",
      "llama_perf_context_print:    graphs reused =         62\n",
      "Llama.generate: 126 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ mq\\n–ø—Ä–æ–ø–∏—à–∏ —Ä–µ–ª–∏–∑–Ω–æ—É—Ç –∏ –≤–µ—Ç–∫–∏ –ø–ª–∑\", \"date\": \"2026-01-20\", \"time\": \"22:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: 12  —è–Ω–≤–∞—Ä—è –∏—Ç–æ–∏–≥\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =      71.89 ms /    16 tokens (    4.49 ms per token,   222.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1572.10 ms /    39 runs   (   40.31 ms per token,    24.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    1669.90 ms /    55 tokens\n",
      "llama_perf_context_print:    graphs reused =         37\n",
      "Llama.generate: 127 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–∏—Ç–æ–∏–≥\", \"date\": \"2026-01-12\", \"time\": \"00:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: 27 —è–Ω–≤–∞—Ä—è –∏—Ç–æ–≥–∏ —Ä–æ–∑—ã–≥—Ä—ã—à–∞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =      92.66 ms /    21 tokens (    4.41 ms per token,   226.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1837.10 ms /    45 runs   (   40.82 ms per token,    24.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1960.23 ms /    66 tokens\n",
      "llama_perf_context_print:    graphs reused =         42\n",
      "Llama.generate: 127 prefix-match hit, remaining 205 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–∏—Ç–æ–≥–∏ —Ä–æ–∑—ã–≥—Ä—ã—à–∞\", \"date\": \"2026-01-27\", \"time\": \"00:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: ‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏è\n",
      "\n",
      "–î–∞—Ä–∏–º –¥–≤–æ–∏–º –ø–û 500‚ÇΩ –Ω–∞ –∫–∞—Ä—Ç—É\n",
      "\n",
      "‚ô•Ô∏è–£ –° –õ –û –í –ò –Ø\n",
      "\n",
      "‚ô•Ô∏è–ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ \n",
      "‚ú® –¢–∞–º –≥–¥–ï –∂–∏–≤—ë—Ç –ª–Æ–ë–æ–≤—å\n",
      "‚ú® –ò–ó–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ \"–ª–ò–°–´\"\n",
      "\n",
      "‚ô•Ô∏è –ù–∞–∂–∞—Ç—å –∫–ù–æ–ø–∫—É —É—á–∞—Å—Ç–≤—É—é\n",
      "\n",
      "–í–µ—Ä–∏—Ç—å –≤ —Å–≤–û—é —É–¥–∞—á—Éüí´\n",
      "‚ô•Ô∏è –í–∞–∂–Ω–æ–µ –ø—Ä–∞–í–∏–ª–æ\n",
      "\n",
      "–£ –ø–û–±–µ–¥–∏—Ç–µ–ª–ï–π –¥–û–õ–∂–Ω–∞ –±–´—Ç—å –∫–∞—Ä—Ç–∞ –†–§ –∏–ª–∏ WB/ozon\n",
      "\n",
      "#—Ä–æ–∑—ã–≥—Ä—ã—à\n",
      "\n",
      "—É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: 138\n",
      "–ü—Ä–ò–∑–æ–≤—ã–• –º–µ—Å—Ç: 2\n",
      "–î–ê—Ç–∞ —Ä–æ–∑—ã–≥—Ä—ã—à–∞: 17:00, 16.02.2026 MSK (10 –¥–Ω–µ–π)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     635.64 ms /   205 tokens (    3.10 ms per token,   322.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8483.30 ms /   209 runs   (   40.59 ms per token,    24.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    9270.49 ms /   414 tokens\n",
      "llama_perf_context_print:    graphs reused =        202\n",
      "Llama.generate: 127 prefix-match hit, remaining 30 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–î–∞—Ä–∏–º –¥–≤–æ–∏–º –ø–û 500‚ÇΩ –Ω–∞ –∫–∞—Ä—Ç—É\\n\\n–£ –° –õ –û –í –ò –Ø\\n\\n–ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞\\n‚ú® –¢–∞–º –≥–¥–ï –∂–∏–≤—ë—Ç –ª–Æ–±–æ–≤—å\\n‚ú® –ò–ó–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ \\\"–ª–ò–°–´\\\"\\n\\n–ù–∞–∂–∞—Ç—å –∫–ù–æ–ø–∫—É —É—á–∞—Å—Ç–≤—É—é\\n\\n–í–µ—Ä–∏—Ç—å –≤ —Å–≤–û—é —É–¥–∞—á—Éüí´\\n\\n–£ –ø–æ–±–µ–¥–∏—Ç–µ–ª–ï–π –¥–û–õ–∂–Ω–∞ –±–´—Ç—å –∫–∞—Ä—Ç–∞ –†–§ –∏–ª–∏ WB/ozon\\n\\n#—Ä–æ–∑—ã–≥—Ä—ã—à\\n\\n—É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: 138\\n–ü—Ä–ò–∑–æ–≤—ã–• –º–µ—Å—Ç: 2\\n–î–ê—Ç–∞ —Ä–æ–∑—ã–≥—Ä—ã—à–∞: 17:00, 16.02.2026 MSK (10 –¥–Ω–µ–π)\", \"date\": \"2026-02-16\", \"time\": \"17:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: –ö–ê–ñ–î–û–ï  1 –ß–ò–°–õ–û –û–ü–õ–ê–ê–¢ –ü–û–ß–¢–´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     125.09 ms /    30 tokens (    4.17 ms per token,   239.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1637.46 ms /    41 runs   (   39.94 ms per token,    25.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1789.82 ms /    71 tokens\n",
      "llama_perf_context_print:    graphs reused =         38\n",
      "Llama.generate: 127 prefix-match hit, remaining 27 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–æ–ø–ª–∞—Ç—É –ø–æ—á—Ç—ã\", \"date\": \"2026-02-01\", \"time\": \"00:00\", \"repeat\": \"monthly\"}\n",
      "--------------------\n",
      "Input: 08.12 –≤ 10:000 –≤–∏–≥–∞–Ω—Ç–æ–ª –∂–µ–Ω–µ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     117.85 ms /    27 tokens (    4.36 ms per token,   229.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1723.61 ms /    42 runs   (   41.04 ms per token,    24.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1869.87 ms /    69 tokens\n",
      "llama_perf_context_print:    graphs reused =         39\n",
      "Llama.generate: 127 prefix-match hit, remaining 27 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–≤–∏–≥–∞–Ω—Ç–æ–ª –∂–µ–Ω–µ\", \"date\": \"2026-02-18\", \"time\": \"10:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: 29 —á–∏—Å–∞–ª –Ω–∞–ø–æ–º–Ω–∏—Ç—å –±–æ—Å—Å—É —á—Ç–æ–± —è –ø–æ–∫–∞–∑–∞–ª –º—É—Ö–∏–Ω—É\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     121.18 ms /    27 tokens (    4.49 ms per token,   222.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1741.80 ms /    43 runs   (   40.51 ms per token,    24.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1891.62 ms /    70 tokens\n",
      "llama_perf_context_print:    graphs reused =         40\n",
      "Llama.generate: 127 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–ø–æ–∫–∞–∑–∞—Ç—å –º—É—Ö–∏–Ω—É\", \"date\": \"2026-02-29\", \"time\": \"00:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: 31  –¥–µ–∫–∞–±—Ä—è –∏—Ç–æ–≥–∏ —Ä–æ–∑—ã–≥—Ä—ã—à–∞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =      95.41 ms /    22 tokens (    4.34 ms per token,   230.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1798.81 ms /    45 runs   (   39.97 ms per token,    25.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1924.00 ms /    67 tokens\n",
      "llama_perf_context_print:    graphs reused =         42\n",
      "Llama.generate: 127 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–∏—Ç–æ–≥–∏ —Ä–æ–∑—ã–≥—Ä—ã—à–∞\", \"date\": \"2026-12-31\", \"time\": \"00:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: 25 —è–Ω–≤–∞—Ä—è, –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –∫—Ä—É–∂–æ—á–∫\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =      91.69 ms /    21 tokens (    4.37 ms per token,   229.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1818.62 ms /    45 runs   (   40.41 ms per token,    24.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1940.23 ms /    66 tokens\n",
      "llama_perf_context_print:    graphs reused =         42\n",
      "Llama.generate: 127 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –∫—Ä—É–∂–æ—á–∫\", \"date\": \"2026-01-25\", \"time\": \"00:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: 18 —è–Ω–≤–∞—Ä—è –∏—Ç–æ–∏–≥ —Ä–æ–∑—ã–≥—Ä—ã—à–∞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =      89.90 ms /    20 tokens (    4.50 ms per token,   222.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1552.99 ms /    39 runs   (   39.82 ms per token,    25.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1668.80 ms /    59 tokens\n",
      "llama_perf_context_print:    graphs reused =         37\n",
      "Llama.generate: 127 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–∏—Ç–æ–∏–≥ —Ä–æ–∑—ã–≥—Ä—ã—à–∞\", \"date\": \"2026-01-18\", \"time\": null, \"repeat\": null}\n",
      "--------------------\n",
      "Input: –í 7:00 –ö–ê–ñ–î–´–ò –î–ï–ù–¨ –ü–û–î–™–ï–ú\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     161.12 ms /    29 tokens (    5.56 ms per token,   179.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1870.36 ms /    47 runs   (   39.79 ms per token,    25.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    2062.44 ms /    76 tokens\n",
      "llama_perf_context_print:    graphs reused =         44\n",
      "Llama.generate: 127 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"date\": null, \"repeat\": \"daily\", \"text\": \"–í 7:00 –ö–ê–ñ–î–´–ò –î–ï–ù–¨ –ü–û–î–™–ï–ú\", \"time\": \"07:00\"}\n",
      "--------------------\n",
      "Input: –∫–∞–∂–¥—ã–∏ –¥–µ–Ω—å 30 –¥–Ω–µ–π –ø–æ–¥—Ä—è–¥ –≤ 8:00 –ø–æ –æ–¥–Ω–æ–π –∫–∞–ø—Å—É–ª–µ –≤ –¥–µ–Ω—å. 30 –¥–Ω–µ–π –ø–æ–¥—Ä—è–¥\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     196.90 ms /    39 tokens (    5.05 ms per token,   198.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2405.14 ms /    60 runs   (   40.09 ms per token,    24.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    2642.42 ms /    99 tokens\n",
      "llama_perf_context_print:    graphs reused =         57\n",
      "Llama.generate: 127 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–∫–∞–∂–¥—ã–∏ –¥–µ–Ω—å 30 –¥–Ω–µ–π –ø–æ–¥—Ä—è–¥ –≤ 8:00 –ø–æ –æ–¥–Ω–æ–π –∫–∞–ø—Å—É–ª–µ –≤ –¥–µ–Ω—å.\", \"date\": \"2026-02-18\", \"time\": \"08:00\", \"repeat\": \"daily\"}\n",
      "--------------------\n",
      "Input: 23 —Ñ–µ–≤—Ä–∞–ª—è –∏—Ç–æ–≥–∏\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =      91.95 ms /    15 tokens (    6.13 ms per token,   163.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1570.27 ms /    39 runs   (   40.26 ms per token,    24.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1688.57 ms /    54 tokens\n",
      "llama_perf_context_print:    graphs reused =         37\n",
      "Llama.generate: 127 prefix-match hit, remaining 43 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–∏—Ç–æ–≥–∏\", \"date\": \"2026-02-23\", \"time\": \"00:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: –ó–∞–≤—Ç—Ä–∞  –≤ 11-00\n",
      "\n",
      "https://www.liveworksheets.com/worksheet/en/english-second-language-esl/348744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     164.49 ms /    43 tokens (    3.83 ms per token,   261.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1836.91 ms /    46 runs   (   39.93 ms per token,    25.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    2033.13 ms /    89 tokens\n",
      "llama_perf_context_print:    graphs reused =         44\n",
      "Llama.generate: 127 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–ó–∞–≤—Ç—Ä–∞ –≤ 11-00\", \"date\": \"2026-02-19\", \"time\": \"11:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: 30 —è–Ω–≤–∞—Ä—è –∏–æ—Ç–≥–∏ —Ä–æ–∑—ã–≥—Ä–´–®–∞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     107.36 ms /    21 tokens (    5.11 ms per token,   195.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1683.03 ms /    42 runs   (   40.07 ms per token,    24.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1818.96 ms /    63 tokens\n",
      "llama_perf_context_print:    graphs reused =         40\n",
      "Llama.generate: 126 prefix-match hit, remaining 210 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"—Ä–æ–∑—ã–≥—Ä–´–®–∞\", \"date\": \"2026-01-30\", \"time\": \"00:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: [–†–µ–ª–∏–∑—ã] \n",
      "https://wiki.domrf.ru/pages/viewpage.action?pageId=267595334\n",
      "\n",
      "–ë–∞–Ω–∫:\n",
      "‚ÄºÔ∏è27.01.2026 –≤ 21:0 (@TurovVal)\n",
      "https://jira.domrf.ru/browse/INTEGRATIO-5021\n",
      "https://jira.domrf.ru/browse/INTEGRATIO-4803\n",
      "–•–§ –ø–æ —ç—Å–∫—Ä–æ—É JurDocIntegrationError + –ª–∏–±–∞ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —Ñ–∞—Å–∞–¥\n",
      "https://jira.domrf.ru/browse/RFC-10003\n",
      "@hopheylalalei –ø—Ä–æ–ø–∏—à–∏ —Ä–µ–ª–∏–∑–Ω–æ—É—Ç (–ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏) –∏ –≤–µ—Ç–∫–∏ –ø–ª–∑ https://wiki.domrf.ru/pages/viewpage.action?pageId=315611369 \n",
      "\n",
      "[athena-facade] [athena-adapter]\n",
      "\n",
      "@alexzodiac @PristTech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     658.93 ms /   210 tokens (    3.14 ms per token,   318.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4365.94 ms /   107 runs   (   40.80 ms per token,    24.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    5097.10 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =        103\n",
      "Llama.generate: 127 prefix-match hit, remaining 195 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"‚ÄºÔ∏è27.01.2026 –≤ 21:0 (@TurovVal) –•–§ –ø–æ —ç—Å–∫—Ä–æ—É JurDocIntegrationError + –ª–∏–±–∞ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —Ñ–∞—Å–∞–¥ @hopheylalalei –ø—Ä–æ–ø–∏—à–∏ —Ä–µ–ª–∏–∑–Ω–æ—É—Ç (–ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏) –∏ –≤–µ—Ç–∫–∏ –ø–ª–∑\", \"date\": \"2026-01-27\", \"time\": \"21:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: [–†–ï–õ–ò–ó–´]\n",
      "WIKI.DOMRF.RU/PAGES/VIEWPAGE.ACTION?PAGEID=267595334\n",
      "\n",
      "–ë–ê–ù–ö:\n",
      "‚ÄºÔ∏è10.02.2026 –í 18.00 (–í–¢–û–†–ù–ò–ö) \n",
      "HTTPS://JIRA.DOMRF.RU/BROWSE/INTEGRATIO-5018\n",
      "CONNECTION_POOL –ü–†–ê–í–ò–ú\n",
      "HTTPS://JIRA.DOMRF.RU/BROWSE/RFC-10152\n",
      "@EXKOT –ü–†–û–ü–ò–®–ò –†–ï–õ–ò–ó–ù–û–£–¢ –ò –í–ï–¢–ö–ò –ü–õ–ó HTTPS://WIKI.DOMRF.RU/PAGES/VIEWPAGE.ACTION?PAGEID=315611369 \n",
      "\n",
      "[CRM-ATHENA] \n",
      "\n",
      "@ALEXZODIAC @PRISTTECH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     601.92 ms /   195 tokens (    3.09 ms per token,   323.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6234.29 ms /   154 runs   (   40.48 ms per token,    24.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    6944.81 ms /   349 tokens\n",
      "llama_perf_context_print:    graphs reused =        149\n",
      "Llama.generate: 126 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–ë–ê–ù–ö:‚ÄºÔ∏è10.02.2026 –í 18.00 (–í–¢–û–†–ù–ò–ö) CONNECTION_POOL –ü–†–ê–í–ò–ú HTTPS://JIRA.DOMRF.RU/BROWSE/INTEGRATIO-5018 RFC-10152 @EXKOT –ü–†–û–ü–ò–®–ò –†–ï–õ–ò–ó–ù–û–£–¢ –ò –í–ï–¢–ö–ò –ü–õ–ó HTTPS://WIKI.DOMRF.RU/PAGES/VIEWPAGE.ACTION?PAGEID=315611369\", \"date\": \"2026-02-10\", \"time\": \"18:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: 28 —è–Ω–≤–∞—Ä—è –∏—Ç–æ–≥–∏ —Ä–æ–∑—ã–≥—Ä—ã—à–∞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =      97.89 ms /    22 tokens (    4.45 ms per token,   224.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1556.80 ms /    39 runs   (   39.92 ms per token,    25.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1681.00 ms /    61 tokens\n",
      "llama_perf_context_print:    graphs reused =         37\n",
      "Llama.generate: 126 prefix-match hit, remaining 301 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–∏—Ç–æ–≥–∏ —Ä–æ–∑—ã–≥—Ä—ã—à–∞\", \"date\": \"2026-01-28\", \"time\": null, \"repeat\": null}\n",
      "--------------------\n",
      "Input: [–†–µ–ª–∏–∑—ã] \n",
      "https://wiki.domrf.ru/pages/viewpAge.action?paGeId=267595334\n",
      "\n",
      "–ë–∞–Ω–∫:\n",
      "‚ÄºÔ∏è18.01.2026 –≤ 10:00 (–≤–æ—Å–∫—Ä)\n",
      "https://jira.domrf.ru/browse/INTEGRATIO-4771\n",
      "https://jira.dOmrf.ru/bRowse/INTEGRATIO-4879\n",
      "–µ–õ–∫-–≠—Å–∫—Ä–æ—É –°–ò–Ω—Ö–†–æ–Ω–∏–ó–∞—Ü–∏–Ø –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ù–∞ + –¥–æ—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–Ω–ï—Ñ–ò—Ü–ò–ê—Ä–æ–≤\n",
      "https://jira.domrf.ru/browse/RFC-9906 \n",
      "@exkot @hopheylalalei —Å–¥–µ–ª–∞–π—Ç–µ —Å—Ç–†–∞–Ω–∏—Ü—É –ø–æ —ç–¢–æ–º—É —Ä–µ–ª–∏–∑—É (–ø–æ –≤–æ–∑–ú–æ–∂–Ω–æ—Å—Ç–∏, –ª–∏–ë–æ —Ä–ø–æ—Å—Ç–æ –æ—Ç–ø–∏—à–∏—Ç–µ –µ—Å—Ç—å –ª–∏ –ö–∞–∫–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª–¨–Ω—ã–µ –∑–ê–¥–∞–ß–∏ –ø–û –ö–æ–Ω—Ñ–∏–≥—É–≥–∏—Ä–æ–≤–∞–Ω–∏—é –≤ —ç—Ç–æ–º —Ä–µ–ª–∏–∑–µ) –∏ –ø—Ä–æ–ø–ò—à–∏—Ç–µ –≤–µ—Ç–∫–∏ –ø–ª–∑ –≤ —Ç–∞–±–ª–∏—á–∫–µ https://wiki.dOmrf.ru/pagEs/viewpAge.Action?pageId=315611369 \n",
      "\n",
      " [elK-facade] [athena-adapter] [athena-facade]\n",
      "\n",
      "@alexzOdiac..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     922.67 ms /   301 tokens (    3.07 ms per token,   326.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2523.44 ms /    62 runs   (   40.70 ms per token,    24.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    3488.19 ms /   363 tokens\n",
      "llama_perf_context_print:    graphs reused =         59\n",
      "Llama.generate: 126 prefix-match hit, remaining 23 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–ë–∞–Ω–∫: —Å–¥–µ–ª–∞–π—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ —ç—Ç–æ–º—É —Ä–µ–ª–∏–∑—É –∏ –ø—Ä–æ–ø–∏—à–∏—Ç–µ –≤–µ—Ç–∫–∏ –≤ —Ç–∞–±–ª–∏—á–∫–µ\", \"date\": \"2026-01-18\", \"time\": \"10:00\", \"repeat\": \"none\"}\n",
      "--------------------\n",
      "Input: 28  —è–Ω–≤–∞—Ä—è –∏—Ç–æ–≥–∏ —Ä–æ–∑–´–≥—Ä—ã—à–∞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     517.78 ms\n",
      "llama_perf_context_print: prompt eval time =     102.91 ms /    23 tokens (    4.47 ms per token,   223.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1799.13 ms /    45 runs   (   39.98 ms per token,    25.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1932.34 ms /    68 tokens\n",
      "llama_perf_context_print:    graphs reused =         42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {\"text\": \"–∏—Ç–æ–≥–∏ —Ä–æ–∑–´–≥—Ä—ã—à–∞\", \"date\": \"2026-01-28\", \"time\": \"00:00\", \"repeat\": \"none\"}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def extract_reminder(text, context_date=\"2026-02-18\"):\n",
    "    if not llm:\n",
    "        return \"Model not loaded.\"\n",
    "    \n",
    "    system_prompt = \"\"\"–¢—ã ‚Äî —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–π.\n",
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç, –¥–∞—Ç—É, –≤—Ä–µ–º—è –∏ –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≤–µ—Ä–Ω—É—Ç—å JSON.\n",
    "–ò—Å–ø–æ–ª—å–∑—É–π —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É (Context Date) –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞—Ç.\n",
    "–§–æ—Ä–º–∞—Ç: {\"text\": \"...\", \"date\": \"YYYY-MM-DD\", \"time\": \"HH:MM\", \"repeat\": \"...\"}\"\"\"\n",
    "    \n",
    "    user_prompt = f\"Context Date: {context_date}\\nMessage: \\\"{text}\\\"\\n\\nJSON:\"\n",
    "    \n",
    "    prompt = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=256,\n",
    "        stop=[\"<|im_end|>\"],\n",
    "        temperature=0.1,\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    return output['choices'][0]['text']\n",
    "\n",
    "# Load random 15 messages from CSV\n",
    "try:\n",
    "    df = pd.read_csv(\"user_messages.csv\")\n",
    "    # Filter for non-empty messages\n",
    "    df = df.dropna(subset=['message_text'])\n",
    "    df = df[df['message_text'].str.len() > 10]\n",
    "    \n",
    "    # Sample 15 random messages\n",
    "    sample_df = df.sample(25)\n",
    "    test_messages = sample_df['message_text'].tolist()\n",
    "    print(f\"Loaded {len(test_messages)} random messages from CSV.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV, using default examples: {e}\")\n",
    "    test_messages = [\n",
    "        \"–ù–∞–ø–æ–º–Ω–∏ –∫—É–ø–∏—Ç—å –º–æ–ª–æ–∫–æ –∑–∞–≤—Ç—Ä–∞ –≤ 10 —É—Ç—Ä–∞\",\n",
    "        \"–í—Å—Ç—Ä–µ—á–∞ —Å –±–æ—Å—Å–æ–º 25 —Ñ–µ–≤—Ä–∞–ª—è –≤ 14:00\",\n",
    "        \"–ö–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 9:00 –ø–∏—Ç—å —Ç–∞–±–ª–µ—Ç–∫–∏\"\n",
    "    ]\n",
    "\n",
    "for msg in test_messages:\n",
    "    if llm:\n",
    "        print(f\"Input: {msg}\")\n",
    "        # Clean up newlines for cleaner output display\n",
    "        print(f\"Output: {extract_reminder(msg)}\")\n",
    "        print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc52834",
   "metadata": {},
   "source": [
    "## Note setup for production\n",
    "\n",
    "For the actual 4GB/4CPU deployment, you simply need:\n",
    "1. The `.gguf` file.\n",
    "2. A lightweight python script using `llama-cpp-python` (like above).\n",
    "3. No heavy dependencies like `torch` or `transformers`. Just `llama-cpp-python`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
