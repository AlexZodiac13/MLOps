[2026-02-03T11:00:42.499+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: model_ab_test_validation.validate_model_ab_test manual__2026-02-03T11:00:42.002960+00:00 [queued]>
[2026-02-03T11:00:42.503+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: model_ab_test_validation.validate_model_ab_test manual__2026-02-03T11:00:42.002960+00:00 [queued]>
[2026-02-03T11:00:42.503+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 2
[2026-02-03T11:00:42.511+0000] {taskinstance.py:2191} INFO - Executing <Task(BashOperator): validate_model_ab_test> on 2026-02-03 11:00:42.002960+00:00
[2026-02-03T11:00:42.515+0000] {standard_task_runner.py:60} INFO - Started process 1807 to run task
[2026-02-03T11:00:42.516+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'model_ab_test_validation', 'validate_model_ab_test', 'manual__2026-02-03T11:00:42.002960+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/ab_test_dag.py', '--cfg-path', '/tmp/tmpaja_4qq1']
[2026-02-03T11:00:42.517+0000] {standard_task_runner.py:88} INFO - Job 4: Subtask validate_model_ab_test
[2026-02-03T11:00:42.540+0000] {task_command.py:423} INFO - Running <TaskInstance: model_ab_test_validation.validate_model_ab_test manual__2026-02-03T11:00:42.002960+00:00 [running]> on host 0c5e6175f949
[2026-02-03T11:00:42.580+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='model_ab_test_validation' AIRFLOW_CTX_TASK_ID='validate_model_ab_test' AIRFLOW_CTX_EXECUTION_DATE='2026-02-03T11:00:42.002960+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2026-02-03T11:00:42.002960+00:00'
[2026-02-03T11:00:42.581+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2026-02-03T11:00:42.581+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'python /opt/***/src/ab_test.py --n-iter 2 --cv 2 --sample-frac 0.05 --auto-deploy']
[2026-02-03T11:00:42.586+0000] {subprocess.py:86} INFO - Output:
[2026-02-03T11:00:48.817+0000] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2026-02-03T11:00:48.817+0000] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2026-02-03T11:00:48.987+0000] {subprocess.py:93} INFO - 26/02/03 11:00:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2026-02-03T11:00:50.269+0000] {subprocess.py:93} INFO - 26/02/03 11:00:50 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
[2026-02-03T11:00:50.269+0000] {subprocess.py:93} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2026-02-03T11:00:50.269+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
[2026-02-03T11:00:50.269+0000] {subprocess.py:93} INFO -     raise PySparkRuntimeError(
[2026-02-03T11:00:50.269+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.8, PySpark cannot run with different minor versions.
[2026-02-03T11:00:50.270+0000] {subprocess.py:93} INFO - Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2026-02-03T11:00:50.270+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:50.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2026-02-03T11:00:50.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2026-02-03T11:00:50.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2026-02-03T11:00:50.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2026-02-03T11:00:50.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2026-02-03T11:00:50.270+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2026-02-03T11:00:50.271+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-03T11:00:50.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
[2026-02-03T11:00:50.273+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-03T11:00:50.273+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-03T11:00:50.273+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-03T11:00:50.283+0000] {subprocess.py:93} INFO - 26/02/03 11:00:50 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (0c5e6175f949 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2026-02-03T11:00:50.283+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
[2026-02-03T11:00:50.283+0000] {subprocess.py:93} INFO -     raise PySparkRuntimeError(
[2026-02-03T11:00:50.284+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.8, PySpark cannot run with different minor versions.
[2026-02-03T11:00:50.284+0000] {subprocess.py:93} INFO - Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2026-02-03T11:00:50.284+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:50.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2026-02-03T11:00:50.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2026-02-03T11:00:50.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2026-02-03T11:00:50.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2026-02-03T11:00:50.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2026-02-03T11:00:50.284+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2026-02-03T11:00:50.285+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2026-02-03T11:00:50.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.285+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2026-02-03T11:00:50.285+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2026-02-03T11:00:50.285+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2026-02-03T11:00:50.285+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2026-02-03T11:00:50.285+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2026-02-03T11:00:50.285+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2026-02-03T11:00:50.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.286+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2026-02-03T11:00:50.286+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2026-02-03T11:00:50.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.286+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2026-02-03T11:00:50.286+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2026-02-03T11:00:50.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2026-02-03T11:00:50.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2026-02-03T11:00:50.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-03T11:00:50.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-03T11:00:50.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-03T11:00:50.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
[2026-02-03T11:00:50.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-03T11:00:50.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-03T11:00:50.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-03T11:00:50.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
[2026-02-03T11:00:50.287+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-03T11:00:50.287+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-03T11:00:50.288+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-03T11:00:50.288+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:50.288+0000] {subprocess.py:93} INFO - 26/02/03 11:00:50 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[2026-02-03T11:00:50.565+0000] {subprocess.py:93} INFO - ============================================================
[2026-02-03T11:00:50.565+0000] {subprocess.py:93} INFO - A/B ТЕСТИРОВАНИЕ МОДЕЛЕЙ
[2026-02-03T11:00:50.565+0000] {subprocess.py:93} INFO - ============================================================
[2026-02-03T11:00:50.565+0000] {subprocess.py:93} INFO - Используется существующий эксперимент: url_classification
[2026-02-03T11:00:50.565+0000] {subprocess.py:93} INFO - Загрузка данных...
[2026-02-03T11:00:50.565+0000] {subprocess.py:93} INFO - Исходный размер датасета: 420464
[2026-02-03T11:00:50.566+0000] {subprocess.py:93} INFO - Размер выборки: 21023
[2026-02-03T11:00:50.566+0000] {subprocess.py:93} INFO - Размер обучающей выборки: 14716
[2026-02-03T11:00:50.566+0000] {subprocess.py:93} INFO - Размер тестовой выборки: 6307
[2026-02-03T11:00:50.566+0000] {subprocess.py:93} INFO - Загрузка производственной модели 'url_classifier'...
[2026-02-03T11:00:50.566+0000] {subprocess.py:93} INFO - Модель 'url_classifier' с алиасом 'champion' успешно загружена
[2026-02-03T11:00:50.566+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:50.566+0000] {subprocess.py:93} INFO - Обучение модели-кандидата...
[2026-02-03T11:00:50.566+0000] {subprocess.py:93} INFO - Поиск оптимальных гиперпараметров...
[2026-02-03T11:00:50.566+0000] {subprocess.py:93} INFO - Fitting 2 folds for each of 2 candidates, totalling 4 fits
[2026-02-03T11:00:50.567+0000] {subprocess.py:93} INFO - Поиск завершен!
[2026-02-03T11:00:50.567+0000] {subprocess.py:93} INFO - Лучший CV F1-score: 0.4827
[2026-02-03T11:00:50.567+0000] {subprocess.py:93} INFO - Лучшие параметры: {'classifier__n_estimators': 30, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 1, 'classifier__max_features': None, 'classifier__max_depth': 10}
[2026-02-03T11:00:50.567+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:50.567+0000] {subprocess.py:93} INFO - A/B ТЕСТИРОВАНИЕ МОДЕЛЕЙ
[2026-02-03T11:00:50.567+0000] {subprocess.py:93} INFO - ==================================================
[2026-02-03T11:00:50.567+0000] {subprocess.py:93} INFO - Оценка производственной модели...
[2026-02-03T11:00:50.567+0000] {subprocess.py:93} INFO - Оценка производственной модели с PySpark...
[2026-02-03T11:00:50.567+0000] {subprocess.py:93} INFO - Запуск Spark Session: ProdModelEval...
[2026-02-03T11:00:50.568+0000] {subprocess.py:93} INFO - Ошибка PySpark оценки: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
[2026-02-03T11:00:50.568+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (0c5e6175f949 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2026-02-03T11:00:50.568+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
[2026-02-03T11:00:50.568+0000] {subprocess.py:93} INFO -     raise PySparkRuntimeError(
[2026-02-03T11:00:50.568+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.8, PySpark cannot run with different minor versions.
[2026-02-03T11:00:50.568+0000] {subprocess.py:93} INFO - Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2026-02-03T11:00:50.568+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:50.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2026-02-03T11:00:50.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2026-02-03T11:00:50.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2026-02-03T11:00:50.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2026-02-03T11:00:50.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2026-02-03T11:00:50.569+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2026-02-03T11:00:50.569+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2026-02-03T11:00:50.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.569+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2026-02-03T11:00:50.569+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2026-02-03T11:00:50.570+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2026-02-03T11:00:50.570+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2026-02-03T11:00:50.570+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2026-02-03T11:00:50.570+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2026-02-03T11:00:50.570+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.570+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2026-02-03T11:00:50.570+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2026-02-03T11:00:50.570+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.571+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2026-02-03T11:00:50.571+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2026-02-03T11:00:50.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2026-02-03T11:00:50.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2026-02-03T11:00:50.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-03T11:00:50.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-03T11:00:50.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-03T11:00:50.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
[2026-02-03T11:00:50.572+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-03T11:00:50.572+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-03T11:00:50.572+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-03T11:00:50.572+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
[2026-02-03T11:00:50.572+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-03T11:00:50.572+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-03T11:00:50.572+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-03T11:00:50.572+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:50.573+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2026-02-03T11:00:50.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
[2026-02-03T11:00:50.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
[2026-02-03T11:00:50.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
[2026-02-03T11:00:50.573+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2026-02-03T11:00:50.573+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2026-02-03T11:00:50.573+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2026-02-03T11:00:50.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
[2026-02-03T11:00:50.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
[2026-02-03T11:00:50.574+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
[2026-02-03T11:00:50.574+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2026-02-03T11:00:50.574+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
[2026-02-03T11:00:50.574+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
[2026-02-03T11:00:50.574+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
[2026-02-03T11:00:50.574+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
[2026-02-03T11:00:50.574+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2026-02-03T11:00:50.574+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
[2026-02-03T11:00:50.575+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
[2026-02-03T11:00:50.575+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
[2026-02-03T11:00:50.575+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
[2026-02-03T11:00:50.575+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)
[2026-02-03T11:00:50.575+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
[2026-02-03T11:00:50.575+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2026-02-03T11:00:50.575+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2026-02-03T11:00:50.575+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2026-02-03T11:00:50.575+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2026-02-03T11:00:50.576+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2026-02-03T11:00:50.576+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2026-02-03T11:00:50.576+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2026-02-03T11:00:50.576+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2026-02-03T11:00:50.576+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2026-02-03T11:00:50.576+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2026-02-03T11:00:50.576+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2026-02-03T11:00:50.576+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-03T11:00:50.576+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO -     raise PySparkRuntimeError(
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.8, PySpark cannot run with different minor versions.
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2026-02-03T11:00:50.577+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-03T11:00:50.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-03T11:00:50.579+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-03T11:00:50.579+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
[2026-02-03T11:00:50.579+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-03T11:00:50.579+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-03T11:00:50.579+0000] {subprocess.py:93} INFO - 	... 1 more
[2026-02-03T11:00:50.579+0000] {subprocess.py:93} INFO - Exception ignored in: <function JavaModelWrapper.__del__ at 0x73e0ae75d940>
[2026-02-03T11:00:50.579+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2026-02-03T11:00:50.579+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/mllib/common.py", line 152, in __del__
[2026-02-03T11:00:50.580+0000] {subprocess.py:93} INFO -     assert self._sc._gateway is not None
[2026-02-03T11:00:50.580+0000] {subprocess.py:93} INFO - AttributeError: 'MulticlassMetrics' object has no attribute '_sc'
[2026-02-03T11:00:51.047+0000] {subprocess.py:93} INFO - 26/02/03 11:00:51 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
[2026-02-03T11:00:51.048+0000] {subprocess.py:93} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2026-02-03T11:00:51.048+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
[2026-02-03T11:00:51.048+0000] {subprocess.py:93} INFO -     raise PySparkRuntimeError(
[2026-02-03T11:00:51.048+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.8, PySpark cannot run with different minor versions.
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2026-02-03T11:00:51.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-03T11:00:51.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - 26/02/03 11:00:51 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (0c5e6175f949 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO -     raise PySparkRuntimeError(
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.8, PySpark cannot run with different minor versions.
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2026-02-03T11:00:51.051+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2026-02-03T11:00:51.052+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2026-02-03T11:00:51.052+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2026-02-03T11:00:51.052+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2026-02-03T11:00:51.052+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2026-02-03T11:00:51.052+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2026-02-03T11:00:51.052+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.052+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2026-02-03T11:00:51.052+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2026-02-03T11:00:51.052+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2026-02-03T11:00:51.053+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2026-02-03T11:00:51.053+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2026-02-03T11:00:51.053+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2026-02-03T11:00:51.053+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.053+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2026-02-03T11:00:51.053+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2026-02-03T11:00:51.053+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.053+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:51.054+0000] {subprocess.py:93} INFO - 26/02/03 11:00:51 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[2026-02-03T11:00:51.256+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - Метрики производственной модели:
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO -   precision: 0.8709
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO -   recall: 0.3881
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO -   f1_score: 0.5370
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO -   auc: 0.6880
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - Оценка модели-кандидата...
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - Оценка модели-кандидата с PySpark...
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - Запуск Spark Session: CandModelEval...
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - Ошибка PySpark оценки: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (0c5e6175f949 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO -     raise PySparkRuntimeError(
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.8, PySpark cannot run with different minor versions.
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2026-02-03T11:00:51.257+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2026-02-03T11:00:51.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.259+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2026-02-03T11:00:51.259+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2026-02-03T11:00:51.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2026-02-03T11:00:51.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2026-02-03T11:00:51.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-03T11:00:51.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-03T11:00:51.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-03T11:00:51.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
[2026-02-03T11:00:51.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-03T11:00:51.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-03T11:00:51.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-03T11:00:51.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
[2026-02-03T11:00:51.260+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-03T11:00:51.260+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-03T11:00:51.260+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-03T11:00:51.260+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:51.260+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2026-02-03T11:00:51.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
[2026-02-03T11:00:51.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
[2026-02-03T11:00:51.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
[2026-02-03T11:00:51.261+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2026-02-03T11:00:51.261+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2026-02-03T11:00:51.261+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2026-02-03T11:00:51.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
[2026-02-03T11:00:51.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
[2026-02-03T11:00:51.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
[2026-02-03T11:00:51.262+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2026-02-03T11:00:51.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
[2026-02-03T11:00:51.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
[2026-02-03T11:00:51.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
[2026-02-03T11:00:51.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
[2026-02-03T11:00:51.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2026-02-03T11:00:51.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
[2026-02-03T11:00:51.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
[2026-02-03T11:00:51.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
[2026-02-03T11:00:51.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
[2026-02-03T11:00:51.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)
[2026-02-03T11:00:51.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
[2026-02-03T11:00:51.263+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2026-02-03T11:00:51.263+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2026-02-03T11:00:51.263+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2026-02-03T11:00:51.263+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2026-02-03T11:00:51.263+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2026-02-03T11:00:51.263+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 1100, in main
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO -     raise PySparkRuntimeError(
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.8, PySpark cannot run with different minor versions.
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:51.264+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2026-02-03T11:00:51.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2026-02-03T11:00:51.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
[2026-02-03T11:00:51.266+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2026-02-03T11:00:51.266+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2026-02-03T11:00:51.266+0000] {subprocess.py:93} INFO - 	... 1 more
[2026-02-03T11:00:51.266+0000] {subprocess.py:93} INFO - Exception ignored in: <function JavaModelWrapper.__del__ at 0x73e0ae75d940>
[2026-02-03T11:00:51.266+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2026-02-03T11:00:51.266+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.8/site-packages/pyspark/mllib/common.py", line 152, in __del__
[2026-02-03T11:00:51.266+0000] {subprocess.py:93} INFO -     assert self._sc._gateway is not None
[2026-02-03T11:00:51.266+0000] {subprocess.py:93} INFO - AttributeError: 'MulticlassMetrics' object has no attribute '_sc'
[2026-02-03T11:00:52.237+0000] {subprocess.py:93} INFO - 2026/02/03 11:00:52 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.
[2026-02-03T11:00:52.237+0000] {subprocess.py:93} INFO - The git executable must be specified in one of the following ways:
[2026-02-03T11:00:52.237+0000] {subprocess.py:93} INFO -     - be included in your $PATH
[2026-02-03T11:00:52.237+0000] {subprocess.py:93} INFO -     - be set via $GIT_PYTHON_GIT_EXECUTABLE
[2026-02-03T11:00:52.237+0000] {subprocess.py:93} INFO -     - explicitly set via git.refresh(<full-path-to-git-executable>)
[2026-02-03T11:00:52.237+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO - All git commands will error until this is rectified.
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO - This initial message can be silenced or aggravated in the future by setting the
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO - $GIT_PYTHON_REFRESH environment variable. Use one of the following values:
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO -     - quiet|q|silence|s|silent|none|n|0: for no message or exception
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO -     - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO -     - error|e|exception|raise|r|2: for a raised exception
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO - Example:
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO -     export GIT_PYTHON_REFRESH=quiet
[2026-02-03T11:00:52.238+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.313+0000] {subprocess.py:93} INFO - 2026/02/03 11:00:52 INFO mlflow.tracking._tracking_service.client: 🏃 View run ab_test_validation at: http://mlflow_server:5000/#/experiments/1/runs/a1949b8dd8034a4d97e54df296fb0b62.
[2026-02-03T11:00:52.313+0000] {subprocess.py:93} INFO - 2026/02/03 11:00:52 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow_server:5000/#/experiments/1.
[2026-02-03T11:00:52.609+0000] {subprocess.py:93} INFO - 2026/02/03 11:00:52 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmplr59c501/model/model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.3.2', 'cloudpickle==3.1.2']. Set logging level to DEBUG to see the full traceback.
[2026-02-03T11:00:52.613+0000] {subprocess.py:93} INFO - 2026/02/03 11:00:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.
[2026-02-03T11:00:52.802+0000] {subprocess.py:93} INFO - 2026/02/03 11:00:52 INFO mlflow.tracking._tracking_service.client: 🏃 View run loud-lark-882 at: http://mlflow_server:5000/#/experiments/1/runs/2f78b7f84f2a4e59a1ca34024c575e8a.
[2026-02-03T11:00:52.802+0000] {subprocess.py:93} INFO - 2026/02/03 11:00:52 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow_server:5000/#/experiments/1.
[2026-02-03T11:00:52.912+0000] {subprocess.py:93} INFO - Registered model 'url_classifier' already exists. Creating a new version of this model...
[2026-02-03T11:00:52.912+0000] {subprocess.py:93} INFO - 2026/02/03 11:00:52 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: url_classifier, version 3
[2026-02-03T11:00:52.930+0000] {subprocess.py:93} INFO - Created version '3' of model 'url_classifier'.
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - Метрики модели-кандидата:
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO -   precision: 0.8709
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO -   recall: 0.3881
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO -   f1_score: 0.5370
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO -   auc: 0.6880
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - Bootstrap анализ (100 итераций)...
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - Bootstrap для производственной модели...
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - Bootstrap для модели-кандидата...
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - Статистическое сравнение (α = 0.01)...
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - Результаты t-теста:
[2026-02-03T11:00:52.931+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO - F1-score:
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Production: 0.5370
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Candidate:  0.5370
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Улучшение:  +0.0000
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   p-value:    1.000000
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Cohen's d:  0.0000
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Незначимое различие при α=0.01
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO - P-score:
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Production: 0.8690
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Candidate:  0.8690
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Улучшение:  +0.0000
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   p-value:    1.000000
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Cohen's d:  0.0000
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -   Незначимое различие при α=0.01
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO - ==================================================
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO - ИТОГОВОЕ РЕШЕНИЕ:
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO - ОСТАВИТЬ текущую модель в Production
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO -    Модель-кандидат не показала улучшения
[2026-02-03T11:00:52.932+0000] {subprocess.py:93} INFO - ==================================================
[2026-02-03T11:00:52.933+0000] {subprocess.py:93} INFO - Логирование результатов A/B теста в MLflow...
[2026-02-03T11:00:52.933+0000] {subprocess.py:93} INFO - Результаты теста залогированы.
[2026-02-03T11:00:52.933+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.933+0000] {subprocess.py:93} INFO - Сохранение модели-кандидата в MLflow...
[2026-02-03T11:00:52.933+0000] {subprocess.py:93} INFO - Модель-кандидат сохранена как версия 3
[2026-02-03T11:00:52.933+0000] {subprocess.py:93} INFO - 
[2026-02-03T11:00:52.933+0000] {subprocess.py:93} INFO - ============================================================
[2026-02-03T11:00:52.933+0000] {subprocess.py:93} INFO - A/B ТЕСТИРОВАНИЕ ЗАВЕРШЕНО
[2026-02-03T11:00:52.933+0000] {subprocess.py:93} INFO - ============================================================
[2026-02-03T11:00:53.616+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2026-02-03T11:00:53.630+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=model_ab_test_validation, task_id=validate_model_ab_test, execution_date=20260203T110042, start_date=20260203T110042, end_date=20260203T110053
[2026-02-03T11:00:53.662+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2026-02-03T11:00:53.672+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
