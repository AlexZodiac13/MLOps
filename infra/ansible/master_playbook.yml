- name: Configure Dataproc Master Node
  hosts: all
  become: yes
  user: ubuntu
  gather_facts: yes
  vars:
    ansible_ssh_pipelining: true  # Ускоряет выполнение
    ansible_ssh_timeout: 7200     # 2 часа SSH timeout
    access_key: "{{ access_key_var }}"
    secret_key: "{{ secret_key_var }}"
    s3_bucket: "{{ s3_bucket_var }}"
    source_s3_bucket: "otus-mlops-source-data"   # NEW: remote source bucket
    hdfs_data_dir: "/user/ubuntu/data"
    spark_jars_dir: "/usr/lib/spark/jars"
    hadoop_aws_jar_url: "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar"
    aws_sdk_jar_url: "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.1026/aws-java-sdk-bundle-1.11.1026.jar"
    copy_single_file: true  # Если true, копируем только 2019-08-22.txt; если false, копируем все файлы
    single_file_name: "2019-08-22.txt"
  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600
      ignore_errors: yes

    - name: Install packages
      apt:
        name: "{{ item }}"
        state: present
      loop:
        - s3cmd
        - jq

    - name: Create directory in HDFS
      command: "hdfs dfs -mkdir -p {{ hdfs_data_dir }}"
      register: mkdir_result
      changed_when: "mkdir_result.rc == 0"
      failed_when: "mkdir_result.rc != 0 and 'File exists' not in mkdir_result.stderr"
      become: no

    - name: Copy SINGLE file from remote S3 to HDFS (when copy_single_file is true)
      command: >
        hadoop distcp -update -m 16
        s3a://{{ source_s3_bucket }}/{{ single_file_name }}
        {{ hdfs_data_dir }}/
      async: 9200
      poll: 30
      become: no
      ignore_errors: yes
      when: copy_single_file

    - name: Copy ALL files from remote S3 to HDFS (when copy_single_file is false)
      shell: |
        echo "=== Начинаем копирование из удалённого S3 в HDFS ==="
        hadoop distcp -update -m 16 \
          s3a://{{ source_s3_bucket }}/ \
          {{ hdfs_data_dir }}/ 2>&1 | \
        while IFS= read -r line; do
          echo "$(date +%H:%M:%S) HDFS: $line"
        done
        echo "=== Копирование из S3 в HDFS завершено ==="
      async: 14400
      poll: 60
      become: no
      args:
        executable: /bin/bash
      ignore_errors: yes
      when: not copy_single_file
      register: distcp_all_result

    - name: Show distcp result for ALL files
      debug:
        msg: "Копирование из удалённого S3 в HDFS: {{ 'завершено' if distcp_all_result is succeeded else 'с ошибками' }}"
      when: not copy_single_file

    - name: Verify that data exists in HDFS
      command: "hdfs dfs -count {{ hdfs_data_dir }}"
      register: hdfs_count
      changed_when: false
      become: no
      failed_when: false

    - name: List files in HDFS directory
      command: "hdfs dfs -ls {{ hdfs_data_dir }}"
      register: hdfs_files
      changed_when: false
      become: no
      async: 120
      poll: 10
      failed_when: false

    - name: Display HDFS files (first 10)
      debug:
        msg: "{{ hdfs_files.stdout_lines[:10] if hdfs_files.stdout_lines is defined else ['No files found'] }}"
      when: hdfs_files is succeeded

    - name: Upgrade pip and setuptools
      pip:
        name:
          - pip
          - setuptools
        state: latest
        executable: pip3
      become: yes

    - name: Install jsonschema first (fix dependencies)
      pip:
        name:
          - "jsonschema>=4.18.0"
          - "packaging>=22.0"
          - "importlib-metadata>=6.0.0"
        state: present
        executable: pip3
        extra_args: "--upgrade --no-cache-dir --disable-pip-version-check"
      become: yes

    - name: Install Python packages
      pip:
        name: 
          - jupyter
          - pyspark
          - pandas
          - matplotlib
          - seaborn
        state: present
        executable: pip3
        extra_args: "--no-cache-dir --disable-pip-version-check"
      become: yes
      async: 900
      poll: 20
      register: pip_install_result

    - name: Show pip install progress
      debug:
        msg: "Python packages installation {{ 'completed' if pip_install_result.finished == 1 else 'in progress' }}"

    - name: Create Jupyter config directory
      file:
        path: "/home/ubuntu/.jupyter"
        state: directory
        mode: '0755'
      become: no

    - name: Generate Jupyter config
      command: jupyter notebook --generate-config
      become: no
      args:
        creates: /home/ubuntu/.jupyter/jupyter_notebook_config.py
      ignore_errors: yes

    - name: Configure Jupyter to accept external connections
      lineinfile:
        path: /home/ubuntu/.jupyter/jupyter_notebook_config.py
        line: "{{ item }}"
        create: yes
      become: no
      loop:
        - "c.NotebookApp.ip = '0.0.0.0'"
        - "c.NotebookApp.port = 8888"
        - "c.NotebookApp.open_browser = False"
        - "c.NotebookApp.allow_root = True"

    - name: Start Jupyter Notebook in background
      shell: |
        nohup jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root > /home/ubuntu/jupyter.log 2>&1 &
        sleep 5
      become: no
      async: 30
      poll: 0

    - name: Wait for Jupyter to start
      wait_for:
        port: 8888
        host: 0.0.0.0
        delay: 10
        timeout: 60

    - name: Get Jupyter token from log
      shell: "grep -o 'token=[0-9a-f]*' /home/ubuntu/jupyter.log | tail -1 | sed 's/token=//' || echo 'NO_TOKEN_FOUND'"
      register: jupyter_token
      become: no
      retries: 3
      delay: 2
      until: jupyter_token.stdout != 'NO_TOKEN_FOUND' and jupyter_token.stdout != ''

    - name: Get public IP address
      shell: "curl -s ifconfig.me"
      register: public_ip
      ignore_errors: yes
      become: no

    - name: Display Jupyter access info
      debug:
        msg: |
          Jupyter Notebook запущен!
          
          Внешний доступ:
             http://{{ public_ip.stdout | default(ansible_default_ipv4.address) }}:8888/?token={{ jupyter_token.stdout }}
          
          Внутренний доступ:
             http://{{ ansible_default_ipv4.address }}:8888/?token={{ jupyter_token.stdout }}
          
          Токен: {{ jupyter_token.stdout }}
          Лог: /home/ubuntu/jupyter.log

    - name: Sanity check - fail only if critical
      assert:
        that:
          - hdfs_count.stdout is defined
          - hdfs_count.stdout | regex_search('^\\s*\\d+\\s+\\d+\\s+\\d+')
          - (hdfs_count.stdout.split() | list)[1] != '0'
        fail_msg: "FATAL: No data in HDFS!"
        success_msg: "Data successfully copied to HDFS"

    - name: Check Jupyter process status
      shell: "pgrep -f jupyter || echo 'not_running'"
      register: jupyter_process_check
      changed_when: false
      become: no

    - name: Warning - Jupyter status
      debug:
        msg: "⚠ Warning: Jupyter process status: {{ jupyter_process_check.stdout | default('unknown') }}"
      when: jupyter_process_check.stdout == 'not_running'

    - name: Ensure SPARK_HOME/jars directory exists
      file:
        path: "{{ spark_jars_dir }}"
        state: directory
        mode: '0755'

    - name: Download hadoop-aws JAR
      get_url:
        url: "{{ hadoop_aws_jar_url }}"
        dest: "{{ spark_jars_dir }}/hadoop-aws-3.3.4.jar"
        mode: '0644'

    - name: Download aws-java-sdk-bundle JAR
      get_url:
        url: "{{ aws_sdk_jar_url }}"
        dest: "{{ spark_jars_dir }}/aws-java-sdk-bundle-1.11.1026.jar"
        mode: '0644'

    - name: Verify JAR files in SPARK_HOME/jars
      command: ls -l "{{ spark_jars_dir }}"
      register: jar_files
      changed_when: false

    - name: Show downloaded JAR files
      debug:
        msg: "{{ jar_files.stdout }}"

