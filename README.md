# MLOps Project

![alt text](image.png)

Здесь лежит код для автоматического развертывания инфраструктуры и обработки данных.
Я настроил пайплайн, который поднимает кластер в Yandex Cloud, обрабатывает сырые данные с помощью Spark и складывает результат в S3.

## Как это работает
Весь процесс управляется через Airflow.
1. **Deploy**: Terraform поднимает виртуальные машины.
2. **Provision**: Ansible настраивает окружение (Hadoop, Spark, Jupyter).
3. **Process**: Spark-job чистит данные и конвертирует их в Parquet.
4. **Destroy**: После работы инфраструктура удаляется для экономии денег.

## Запуск
1. Установить Terraform и YC CLI.
2. В папке `airflow` выполнить `terraform apply`. Это создаст Managed Airflow и загрузит DAG'и.
3. В интерфейсе Airflow запустить `import_variables` для настройки.
4. Включить DAG `end_to_end_cycle` — он будет запускать полный цикл обработки каждые 30 минут.

## Структура репозитория
* `airflow/` — конфигурация Terraform для Managed Airflow.
* `dags/` — скрипты пайплайнов (деплой, обработка, очистка).
* `infra/` — Terraform и Ansible для вычислительного кластера.
* `notebooks/` — черновики и эксперименты с данными.

